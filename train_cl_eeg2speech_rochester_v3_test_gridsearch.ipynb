{"cells":[{"cell_type":"markdown","metadata":{"id":"VhCK252zNjUG"},"source":["## COLAB TOOLS"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20917,"status":"ok","timestamp":1677745275694,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"v-pxnK4OaI-b","outputId":"b391254f-e5c8-4df8-8d85-ea744d3b4809"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":554,"referenced_widgets":["18b03f99d4eb4147a85fb45f03670b42","80bf39e46c8846a191e3250accc813ef"]},"executionInfo":{"elapsed":8068,"status":"ok","timestamp":1677745283758,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"ZEopbadxbJH0","outputId":"89759fde-50ac-420d-d2a8-1cdf0d515417"},"outputs":[{"output_type":"stream","name":"stdout","text":["['train_cl_eeg2speech_rochester_v3_test_gridsearch.ipynb', '.git', '.DS_Store', '.gitignore', 'EEG', 'LICENSE', 'train_cl_eeg2speech_rochester_v1.ipynb', 'train_cl_eeg2speech_rochester_v2.ipynb', '.ipynb_checkpoints', 'train_cl_eeg2speech_rochester_v3_test_old.ipynb', 'runs', 'train_cl_eeg2speech_rochester_v3_test.ipynb', 'train_cl_eeg2speech_rochester_v4_gridseaerch.ipynb', 'train_cl_eeg2speech_2.ipynb', 'train_cl_eeg2speech_rochester_subj_2.ipynb', 'README.md', 'train_eeg2speech_rochester.ipynb', 'train_cl_eeg2speech_rochester_v3.ipynb']\n"]},{"output_type":"stream","name":"stderr","text":["WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n","Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n","To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n"]},{"output_type":"display_data","data":{"text/plain":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Collecting mne\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Collecting mne\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["  Downloading mne-1.3.1-py3-none-any.whl (7.6 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Downloading mne-1.3.1-py3-none-any.whl (7.6 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Output()"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18b03f99d4eb4147a85fb45f03670b42"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":[],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.8/dist-packages (from mne) (1.6.0)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: pooch&gt;=1.5 in /usr/local/lib/python3.8/dist-packages (from mne) (1.6.0)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from mne) (3.5.3)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from mne) (3.5.3)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from mne) (23.0)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from mne) (23.0)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.8/dist-packages (from mne) (1.22.4)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: numpy&gt;=1.15.4 in /usr/local/lib/python3.8/dist-packages (from mne) (1.22.4)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from mne) (3.1.2)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from mne) (3.1.2)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from mne) (4.4.2)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from mne) (4.4.2)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from mne) (1.7.3)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: scipy&gt;=1.1.0 in /usr/local/lib/python3.8/dist-packages (from mne) (1.7.3)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from mne) (4.64.1)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from mne) (4.64.1)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.8/dist-packages (from pooch>=1.5->mne) (1.4.4)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: appdirs&gt;=1.3.0 in /usr/local/lib/python3.8/dist-packages (from pooch&gt;=1.5-&gt;mne) (1.4.4)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from pooch>=1.5->mne) (2.25.1)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: requests&gt;=2.19.0 in /usr/local/lib/python3.8/dist-packages (from pooch&gt;=1.5-&gt;mne) (2.25.1)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->mne) (2.1.2)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2-&gt;mne) (2.1.2)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mne) (8.4.0)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: pillow&gt;=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib-&gt;mne) (8.4.0)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mne) (2.8.2)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: python-dateutil&gt;=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib-&gt;mne) (2.8.2)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mne) (0.11.0)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib-&gt;mne) (0.11.0)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mne) (4.38.0)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: fonttools&gt;=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib-&gt;mne) (4.38.0)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mne) (3.0.9)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: pyparsing&gt;=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib-&gt;mne) (3.0.9)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mne) (1.4.4)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib-&gt;mne) (1.4.4)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7->matplotlib->mne) (1.15.0)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil&gt;=2.7-&gt;matplotlib-&gt;mne) (1.15.0)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2022.12.7)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests&gt;=2.19.0-&gt;pooch&gt;=1.5-&gt;mne) (2022.12.7)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (4.0.0)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: chardet&lt;5,&gt;=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests&gt;=2.19.0-&gt;pooch&gt;=1.5-&gt;mne) (4.0.0)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (1.26.14)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests&gt;=2.19.0-&gt;pooch&gt;=1.5-&gt;mne) (1.26.14)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.10)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.8/dist-packages (from requests&gt;=2.19.0-&gt;pooch&gt;=1.5-&gt;mne) (2.10)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Installing collected packages: mne\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Installing collected packages: mne\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Successfully installed mne-1.3.1\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Successfully installed mne-1.3.1\n","</pre>\n"]},"metadata":{}}],"source":["\n","import os\n","import sys\n","\n","GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = \"Colab Notebooks/prj_neuroread_analysis/neuroread/\"\n","GOOGLE_DRIVE_PATH = os.path.join(\"/content\", \"drive\", \"MyDrive\", GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n","print(os.listdir(GOOGLE_DRIVE_PATH))\n","\n","# Add to sys so we can import .py files.\n","sys.path.append(GOOGLE_DRIVE_PATH)\n","os.chdir(GOOGLE_DRIVE_PATH)\n","\n","# Install unavailable packages\n","import pip\n","def import_or_install(package):\n","    try:\n","        __import__(package)\n","    except ImportError:\n","        pip.main(['install', package])\n","\n","import_or_install(\"mne\")\n"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":290,"status":"ok","timestamp":1677745284044,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"M0LuZowEbY29"},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1414,"status":"ok","timestamp":1677745285452,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"HTqIrJcOaQLu","outputId":"cdcfc642-2bfc-4f5d-c218-702e3a420116"},"outputs":[{"output_type":"stream","name":"stdout","text":["Your runtime has 89.6 gigabytes of available RAM\n","\n","Thu Mar  2 08:21:24 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   35C    P0    51W / 400W |      0MiB / 40960MiB |      0%      Default |\n","|                               |                      |             Disabled |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n"," print('Not connected to a GPU')\n","else:\n"," print(gpu_info)"]},{"cell_type":"markdown","metadata":{"id":"aXEXbz7ENjUM"},"source":["## Main code"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4873,"status":"ok","timestamp":1677745290323,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"gGggzCoKZQeu","outputId":"eb2f89c6-a26d-4f49-84ff-f5dff629041c"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda:0\n"]}],"source":["import os, sys, glob\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","\n","import numpy as np\n","\n","import mne\n","\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import time\n","\n","from torchsummary import summary\n","from torch.utils.tensorboard import SummaryWriter\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1677745290324,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"GYgZe_3OQniG"},"outputs":[],"source":["def eval_model_cl(dl, model, device=torch.device('cpu'), verbose=True):\n","    \"\"\" \n","    This function calculates the loss on data, setting backward gradients and batchnorm\n","    off. This function is written for contrasting learning where the model takes in two\n","    inputs.\n","\n","    Args:\n","\n","    Returns:\n","      loss_test: Mean loss of all test samples (scalar)\n","\n","    \"\"\"\n","    losses, losses_X1, losses_X2 = [], [], []\n","    model.to(device)  # inplace for model\n","    # Set the model in evaluation mode\n","    model.eval()\n","\n","    with torch.no_grad():\n","        for idx_batch, (X1b, X2b) in enumerate(dl):\n","\n","            X1b = X1b.to(device)\n","            X2b = X2b.to(device)\n","\n","            X1b_features, X2b_features, logit_sc = model(X1b, X2b)\n","\n","            # Normalize features\n","            X1b_f_n = X1b_features / X1b_features.norm(dim=1, keepdim=True)\n","            X2b_f_n = X2b_features / X2b_features.norm(dim=1, keepdim=True)\n","\n","            logits_per_X1 = logit_sc * X1b_f_n @ X2b_f_n.t()\n","            logits_per_X2 = logits_per_X1.t()\n","\n","            # Number of labels equals to the 1st dimension of X1b\n","            labels = torch.arange(X1b.shape[0], device=device)\n","\n","            # Batch Loss \n","            loss_X1 = F.cross_entropy(logits_per_X1, labels)\n","            loss_X2 = F.cross_entropy(logits_per_X2, labels)\n","            loss_batch   = (loss_X1 + loss_X2) / 2\n","            losses.append(loss_batch.item())\n","            losses_X1.append(loss_X1.item())\n","            losses_X2.append(loss_X2.item())\n","\n","        # Epoch loss (mean of batch losses)\n","        loss  = sum(losses) / len(losses)\n","        loss_X1 = sum(losses_X1) / len(losses_X1)\n","        loss_X2 = sum(losses_X2) / len(losses_X2)\n","\n","        if verbose:\n","          print(f\"====> Validation loss: {loss:.4f},  X1 loss: {loss_X1:.4f}   X2 loss: {loss_X2:.4f}\")\n","\n","        return loss, loss_X1, loss_X2\n"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1677745290325,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"EaAsZ-SLZQey"},"outputs":[],"source":["def unfold_raw(raw, window_size=None, stride=None):\n","    \"\"\"\n","    This function unfolds raw MNE object into a list of raw objects\n","    Args:\n","        raw: a raw MNE object cropped by rejecting bad segments.\n","    Returns:\n","        raw_unfolded: a raw MNE object unfolded by applying a sliding window.\n","    \"\"\"\n","    if window_size is None:\n","        window_size = int(5 * raw.info['sfreq'])\n","    if stride is None:\n","        stride = window_size\n","    nchans = len(raw.ch_names)\n","    sig = torch.tensor(raw.get_data(), dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n","    sig_unf = F.unfold(sig, (nchans, window_size), stride=stride , padding=0)\n","    sig_unf = sig_unf.permute(0, 2, 1).reshape(-1, sig_unf.shape[-1], nchans, window_size)\n","    return sig_unf"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1677745290325,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"1cf9EoF3ZQey"},"outputs":[],"source":["def rm_repeated_annotations(raw):\n","    \"\"\"This functions taskes in raw MNE obejct and removes repeated annotations\"\"\"\n","    annots = raw.annotations.copy()\n","    annots_drop = []\n","    for k in annots:\n","        annots_drop.extend([k for kk in annots if (k['onset'] > kk['onset']) and (k['onset']+k['duration'] < kk['onset']+kk['duration']) ])\n","\n","    annots_updated = [i for i in annots if i not in annots_drop]\n","    onsets = [i['onset'] for i in annots_updated]\n","    durations = [i['duration'] for i in annots_updated]\n","    descriptions = [i['description'] for i in annots_updated]\n","    print('Initial num of annots: %d  Num of removed annots: %d  Num of retained annots:  %d' % (len(annots), len(annots_drop), len(annots_updated)))\n","    print(f' New annots: {annots_updated}')\n","    raw.set_annotations(mne.Annotations(onsets, durations, descriptions) ) \n","    return raw"]},{"cell_type":"markdown","metadata":{"id":"q4sGJAdFZQez"},"source":["## Read Data"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1677745290326,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"KdQFfHcJZQe0","outputId":"0e5634cf-fb0a-4953-dd75-c523ada41f20"},"outputs":[{"output_type":"stream","name":"stdout","text":["-------------------------------------\n","window_size: 640  stride_size_test: 640\n","data_path: ../outputs/rochester_data/natural_speech\n"]}],"source":["subj_ids = list(range(1, 11))\n","fs = 128\n","window_size = int(5 * fs)\n","stride_size_train, stride_size_val, stride_size_test = int(2.5 * fs), int(5 * fs), int(5 * fs)\n","n_channs = 129 # 128 for eeg, 1 for env\n","batch_size = int(32)\n","print('-------------------------------------')\n","print(f'window_size: {window_size}  stride_size_test: {stride_size_test}')\n","\n","dataset_name = ['rochester_data', 'natural_speech']\n","outputs_path = f'../outputs/'\n","data_path = os.path.join(outputs_path, dataset_name[0], dataset_name[1])\n","after_ica_path = os.path.join(data_path, 'after_ica_raw')\n","print(f'data_path: {data_path}')"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":141504,"status":"ok","timestamp":1677745431822,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"fIhJi5IxZQe1","outputId":"1f9b913c-4640-46e3-b9ff-6a03a511af50"},"outputs":[{"output_type":"stream","name":"stdout","text":["Opening raw data file ../outputs/rochester_data/natural_speech/after_ica_raw/subj_1_after_ica_raw.fif...\n","    Range : 0 ... 464571 =      0.000 ...  3629.461 secs\n","Ready.\n","Reading 0 ... 464571  =      0.000 ...  3629.461 secs...\n","Initial num of annots: 48  Num of removed annots: 19  Num of retained annots:  29\n"," New annots: [OrderedDict([('onset', 0.0), ('duration', 0.0), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 176.559097), ('duration', 2.240447998046875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 357.084473), ('duration', 2.24041748046875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 537.212158), ('duration', 2.0118408203125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 718.890503), ('duration', 2.67486572265625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 899.02771), ('duration', 2.05755615234375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 983.746643), ('duration', 6.58416748046875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1051.039551), ('duration', 1.8746337890625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1100.719482), ('duration', 6.4012451171875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1265.987061), ('duration', 3.269287109375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1437.989502), ('duration', 2.080322265625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1611.904297), ('duration', 11.796630859375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1806.480591), ('duration', 3.81787109375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1977.884644), ('duration', 8.5731201171875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2058.135986), ('duration', 5.052490234375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2161.450928), ('duration', 3.2236328125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2345.70874), ('duration', 1.8291015625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2523.632324), ('duration', 5.578369140625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2656.933838), ('duration', 3.246337890625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2699.549561), ('duration', 15.7060546875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2892.681641), ('duration', 3.497802734375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2977.071045), ('duration', 1.64599609375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3079.503906), ('duration', 8.321533203125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3220.774658), ('duration', 3.017822265625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3257.32251), ('duration', 10.927978515625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3327.002441), ('duration', 6.584228515625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3438.776123), ('duration', 5.806884765625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3480.157471), ('duration', 12.139404296875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3607.381592), ('duration', 10.49365234375), ('description', 'bad'), ('orig_time', None)])]\n","-------------------------------------\n","N train: 26  N val: 1  N test: 1\n","Opening raw data file ../outputs/rochester_data/natural_speech/after_ica_raw/subj_2_after_ica_raw.fif...\n","    Range : 0 ... 464571 =      0.000 ...  3629.461 secs\n","Ready.\n","Reading 0 ... 464571  =      0.000 ...  3629.461 secs...\n","Initial num of annots: 65  Num of removed annots: 19  Num of retained annots:  46\n"," New annots: [OrderedDict([('onset', 0.0), ('duration', 0.0), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 177.009033), ('duration', 4.4808807373046875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 233.547455), ('duration', 3.1549072265625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 256.64325), ('duration', 4.549468994140625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 357.877777), ('duration', 1.554595947265625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 419.798157), ('duration', 4.18365478515625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 521.238403), ('duration', 3.72650146484375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 536.986145), ('duration', 8.36737060546875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 591.58136), ('duration', 12.36810302734375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 684.541626), ('duration', 37.07598876953125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 731.651611), ('duration', 0.9830322265625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 760.453308), ('duration', 5.80682373046875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 895.355164), ('duration', 14.21990966796875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 955.299988), ('duration', 3.9779052734375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1010.325623), ('duration', 5.5782470703125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1047.856567), ('duration', 3.0177001953125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1101.390625), ('duration', 4.892333984375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1266.681152), ('duration', 8.984619140625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1304.783569), ('duration', 3.2464599609375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1385.044556), ('duration', 3.749267578125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1438.464355), ('duration', 2.171875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1613.947266), ('duration', 13.3740234375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1636.351562), ('duration', 1.600341796875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1650.508545), ('duration', 8.5731201171875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1683.526245), ('duration', 6.5384521484375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1726.90979), ('duration', 6.2640380859375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1807.079224), ('duration', 13.076904296875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1982.602295), ('duration', 1.348876953125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1991.061035), ('duration', 2.126220703125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2049.411133), ('duration', 4.503662109375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2136.050537), ('duration', 4.732421875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2162.68042), ('duration', 11.659423828125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2234.577881), ('duration', 7.315673828125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2345.926758), ('duration', 14.63134765625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2523.343262), ('duration', 9.076171875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2546.635498), ('duration', 3.177734375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2707.869873), ('duration', 15.68310546875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2893.512939), ('duration', 8.481689453125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3081.322021), ('duration', 2.468994140625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3087.928955), ('duration', 6.835693359375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3116.119141), ('duration', 3.886474609375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3257.709961), ('duration', 6.652587890625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3280.65918), ('duration', 5.71533203125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3440.005615), ('duration', 0.914306640625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3452.968018), ('duration', 7.29296875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3604.318115), ('duration', 3.817138671875), ('description', 'bad'), ('orig_time', None)])]\n","-------------------------------------\n","N train: 68  N val: 2  N test: 2\n","Opening raw data file ../outputs/rochester_data/natural_speech/after_ica_raw/subj_3_after_ica_raw.fif...\n","    Range : 0 ... 464571 =      0.000 ...  3629.461 secs\n","Ready.\n","Reading 0 ... 464571  =      0.000 ...  3629.461 secs...\n","Initial num of annots: 47  Num of removed annots: 19  Num of retained annots:  28\n"," New annots: [OrderedDict([('onset', 0.0), ('duration', 0.0), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 43.135948), ('duration', 4.115089416503906), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 121.796593), ('duration', 5.6010894775390625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 176.748077), ('duration', 1.760345458984375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 275.317291), ('duration', 2.994842529296875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 357.205322), ('duration', 2.126129150390625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 537.65863), ('duration', 1.89752197265625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 718.599854), ('duration', 2.12615966796875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 898.988525), ('duration', 1.6231689453125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1033.410278), ('duration', 0.9830322265625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1101.293579), ('duration', 1.7603759765625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1266.313721), ('duration', 3.56640625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1307.502563), ('duration', 0.86865234375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1438.074097), ('duration', 2.4461669921875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1583.876953), ('duration', 0.3658447265625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1613.867554), ('duration', 1.9432373046875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1806.839478), ('duration', 1.9661865234375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1982.358643), ('duration', 2.1947021484375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2162.061523), ('duration', 2.14892578125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2345.888916), ('duration', 0.8916015625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2523.831299), ('duration', 2.92626953125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2707.159668), ('duration', 2.01171875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2790.049561), ('duration', 9.510498046875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2893.086426), ('duration', 2.400390625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3081.215576), ('duration', 1.874755859375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3261.645996), ('duration', 2.14892578125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3439.272217), ('duration', 2.42333984375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3525.888916), ('duration', 1.87451171875), ('description', 'bad'), ('orig_time', None)])]\n","-------------------------------------\n","N train: 93  N val: 3  N test: 3\n","Opening raw data file ../outputs/rochester_data/natural_speech/after_ica_raw/subj_4_after_ica_raw.fif...\n","    Range : 0 ... 464394 =      0.000 ...  3628.078 secs\n","Ready.\n","Reading 0 ... 464394  =      0.000 ...  3628.078 secs...\n","Initial num of annots: 41  Num of removed annots: 19  Num of retained annots:  22\n"," New annots: [OrderedDict([('onset', 0.0), ('duration', 0.0), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 176.316391), ('duration', 2.4461822509765625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 357.619507), ('duration', 1.074493408203125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 537.230835), ('duration', 1.851806640625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 718.968323), ('duration', 4.89239501953125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 898.236755), ('duration', 2.56048583984375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1100.372314), ('duration', 2.5833740234375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1265.114136), ('duration', 4.6181640625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1437.281372), ('duration', 1.874755859375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1613.327759), ('duration', 1.783203125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1806.417358), ('duration', 1.2344970703125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1981.789795), ('duration', 2.720458984375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2161.278076), ('duration', 4.8466796875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2344.500732), ('duration', 1.80615234375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2523.130127), ('duration', 2.194580078125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2706.033447), ('duration', 2.743408203125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2876.088623), ('duration', 1.6689453125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2892.718506), ('duration', 0.822998046875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3079.622803), ('duration', 2.217529296875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3261.026855), ('duration', 6.172607421875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3438.603516), ('duration', 0.822998046875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3535.585693), ('duration', 2.720458984375), ('description', 'bad'), ('orig_time', None)])]\n","-------------------------------------\n","N train: 112  N val: 4  N test: 4\n","Opening raw data file ../outputs/rochester_data/natural_speech/after_ica_raw/subj_5_after_ica_raw.fif...\n","    Range : 0 ... 464571 =      0.000 ...  3629.461 secs\n","Ready.\n","Reading 0 ... 464571  =      0.000 ...  3629.461 secs...\n","Initial num of annots: 41  Num of removed annots: 19  Num of retained annots:  22\n"," New annots: [OrderedDict([('onset', 0.0), ('duration', 0.0), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 177.408173), ('duration', 0.270477294921875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 358.121857), ('duration', 0.270477294921875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 537.280273), ('duration', 1.9835205078125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 718.369812), ('duration', 2.3441162109375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 899.271362), ('duration', 1.3974609375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1101.870972), ('duration', 0.4508056640625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1267.272949), ('duration', 1.4425048828125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1438.279663), ('duration', 1.126953125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1614.049805), ('duration', 1.8707275390625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1701.652954), ('duration', 0.96923828125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1806.822144), ('duration', 2.434326171875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1982.066284), ('duration', 2.186279296875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2161.976074), ('duration', 1.983642578125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2345.484619), ('duration', 1.870849609375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2524.411133), ('duration', 0.541015625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2707.077393), ('duration', 2.569580078125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2893.742432), ('duration', 0.8564453125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3081.496094), ('duration', 0.653564453125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3229.895752), ('duration', 1.84814453125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3261.698975), ('duration', 2.727294921875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3439.993408), ('duration', 0.901611328125), ('description', 'bad'), ('orig_time', None)])]\n","-------------------------------------\n","N train: 131  N val: 5  N test: 5\n","Opening raw data file ../outputs/rochester_data/natural_speech/after_ica_raw/subj_6_after_ica_raw.fif...\n","    Range : 0 ... 464571 =      0.000 ...  3629.461 secs\n","Ready.\n","Reading 0 ... 464571  =      0.000 ...  3629.461 secs...\n","Initial num of annots: 58  Num of removed annots: 19  Num of retained annots:  39\n"," New annots: [OrderedDict([('onset', 0.0), ('duration', 0.0), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 176.775345), ('duration', 1.6688995361328125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 282.102112), ('duration', 2.56048583984375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 357.346893), ('duration', 2.263275146484375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 523.917664), ('duration', 2.1260986328125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 536.834473), ('duration', 2.56048583984375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 718.924316), ('duration', 5.0753173828125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 757.242065), ('duration', 2.65191650390625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 837.127747), ('duration', 3.177734375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 879.596802), ('duration', 2.5147705078125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 897.584961), ('duration', 4.526611328125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1101.219971), ('duration', 2.0574951171875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1218.668945), ('duration', 2.4461669921875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1267.177368), ('duration', 1.7603759765625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1405.632324), ('duration', 2.3089599609375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1438.297607), ('duration', 1.8746337890625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1469.58728), ('duration', 2.03466796875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1497.78125), ('duration', 2.4461669921875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1554.397705), ('duration', 10.470703125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1608.48999), ('duration', 8.39013671875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1634.155762), ('duration', 2.468994140625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1747.676392), ('duration', 4.3209228515625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1807.026855), ('duration', 1.6002197265625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1982.125), ('duration', 2.7890625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2036.889648), ('duration', 4.7322998046875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2122.203125), ('duration', 2.03466796875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2162.363037), ('duration', 1.623291015625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2253.597656), ('duration', 2.491943359375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2345.462646), ('duration', 2.034912109375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2487.927979), ('duration', 2.42333984375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2524.04541), ('duration', 1.92041015625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2603.16333), ('duration', 1.8974609375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2706.935303), ('duration', 2.743408203125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2893.359375), ('duration', 2.0576171875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3080.958984), ('duration', 1.783203125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3111.987793), ('duration', 0.64013671875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3261.530518), ('duration', 2.8349609375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3403.469971), ('duration', 1.1201171875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3439.724609), ('duration', 1.028564453125), ('description', 'bad'), ('orig_time', None)])]\n","-------------------------------------\n","N train: 167  N val: 6  N test: 6\n","Opening raw data file ../outputs/rochester_data/natural_speech/after_ica_raw/subj_7_after_ica_raw.fif...\n","    Range : 0 ... 464571 =      0.000 ...  3629.461 secs\n","Ready.\n","Reading 0 ... 464571  =      0.000 ...  3629.461 secs...\n","Initial num of annots: 74  Num of removed annots: 18  Num of retained annots:  56\n"," New annots: [OrderedDict([('onset', 0.0), ('duration', 0.0), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 176.368912), ('duration', 2.6062164306640625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 202.71106), ('duration', 3.5206756591796875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 315.806854), ('duration', 0.0), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 356.574677), ('duration', 2.903411865234375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 534.393311), ('duration', 6.35552978515625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 718.769409), ('duration', 1.44024658203125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 899.308594), ('duration', 2.5147705078125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 981.924255), ('duration', 1.8289794921875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 995.180115), ('duration', 2.53765869140625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1054.425659), ('duration', 13.945556640625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1091.636597), ('duration', 3.54345703125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1100.598267), ('duration', 3.2235107421875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1187.123291), ('duration', 1.851806640625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1267.292847), ('duration', 1.2574462890625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1323.053955), ('duration', 1.5089111328125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1438.083496), ('duration', 1.8289794921875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1504.320801), ('duration', 2.354736328125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1536.858521), ('duration', 5.1666259765625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1614.278198), ('duration', 1.9432373046875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1668.210327), ('duration', 2.766357421875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1713.217041), ('duration', 1.80615234375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1729.60498), ('duration', 3.1319580078125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1784.040039), ('duration', 8.756103515625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1805.663208), ('duration', 3.7950439453125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1982.498901), ('duration', 2.0574951171875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2096.175781), ('duration', 2.37744140625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2157.560791), ('duration', 10.72216796875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2288.804688), ('duration', 3.955078125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2330.930908), ('duration', 2.468994140625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2345.640381), ('duration', 1.600341796875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2363.354248), ('duration', 2.080322265625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2398.324707), ('duration', 2.354736328125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2523.527588), ('duration', 2.53759765625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2550.565186), ('duration', 6.149658203125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2706.920654), ('duration', 2.857666015625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2893.047607), ('duration', 2.26318359375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2912.270264), ('duration', 3.2919921875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2929.330566), ('duration', 3.08642578125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2943.047607), ('duration', 4.09228515625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2969.869873), ('duration', 3.497802734375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2992.805664), ('duration', 3.246337890625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3006.655762), ('duration', 2.743408203125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3080.669922), ('duration', 3.04052734375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3104.24585), ('duration', 2.171875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3139.366943), ('duration', 1.463134765625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3160.199219), ('duration', 6.67578125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3224.72583), ('duration', 9.05322265625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3261.845459), ('duration', 2.6748046875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3312.975098), ('duration', 3.132080078125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3358.553223), ('duration', 2.8349609375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3393.308594), ('duration', 2.080322265625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3432.110352), ('duration', 2.583251953125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3440.40625), ('duration', 0.0), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3503.788574), ('duration', 3.360595703125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3555.731934), ('duration', 7.33837890625), ('description', 'bad'), ('orig_time', None)])]\n","-------------------------------------\n","N train: 220  N val: 7  N test: 7\n","Opening raw data file ../outputs/rochester_data/natural_speech/after_ica_raw/subj_8_after_ica_raw.fif...\n","    Range : 0 ... 464571 =      0.000 ...  3629.461 secs\n","Ready.\n","Reading 0 ... 464571  =      0.000 ...  3629.461 secs...\n","Initial num of annots: 42  Num of removed annots: 19  Num of retained annots:  23\n"," New annots: [OrderedDict([('onset', 0.0), ('duration', 0.0), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 176.14798), ('duration', 2.926300048828125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 357.06424), ('duration', 2.2633056640625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 371.421326), ('duration', 5.235321044921875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 536.955017), ('duration', 4.4808349609375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 718.692444), ('duration', 2.37762451171875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 898.715332), ('duration', 2.65191650390625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1101.184326), ('duration', 2.19482421875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1266.598633), ('duration', 2.743408203125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1437.696045), ('duration', 5.6468505859375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1613.613281), ('duration', 11.3165283203125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1806.690186), ('duration', 2.53759765625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1982.835938), ('duration', 1.348876953125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2162.807617), ('duration', 0.8916015625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2344.188721), ('duration', 3.269287109375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2377.503662), ('duration', 2.92626953125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2523.80957), ('duration', 1.874755859375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2707.339844), ('duration', 1.87451171875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2716.461426), ('duration', 2.240478515625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2888.094238), ('duration', 7.0185546875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3080.641357), ('duration', 2.2861328125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3261.550049), ('duration', 2.652099609375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3439.231689), ('duration', 2.19482421875), ('description', 'bad'), ('orig_time', None)])]\n","-------------------------------------\n","N train: 240  N val: 8  N test: 8\n","Opening raw data file ../outputs/rochester_data/natural_speech/after_ica_raw/subj_9_after_ica_raw.fif...\n","    Range : 0 ... 464396 =      0.000 ...  3628.094 secs\n","Ready.\n","Reading 0 ... 464396  =      0.000 ...  3628.094 secs...\n","Initial num of annots: 50  Num of removed annots: 19  Num of retained annots:  31\n"," New annots: [OrderedDict([('onset', 0.0), ('duration', 0.0), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 130.882126), ('duration', 1.5317230224609375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 175.55928), ('duration', 3.543548583984375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 264.037018), ('duration', 2.994873046875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 356.953827), ('duration', 2.42333984375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 537.136719), ('duration', 2.60626220703125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 718.279785), ('duration', 2.19476318359375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 805.422119), ('duration', 2.03466796875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 863.707458), ('duration', 1.5089111328125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 898.531311), ('duration', 2.1947021484375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 942.221497), ('duration', 1.16595458984375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1100.621216), ('duration', 2.103271484375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1266.725952), ('duration', 1.2344970703125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1361.294312), ('duration', 1.851806640625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1436.863159), ('duration', 3.3377685546875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1612.4375), ('duration', 5.2352294921875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1627.293579), ('duration', 2.6519775390625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1664.12915), ('duration', 145.87548828125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1982.199341), ('duration', 14.970458984375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2160.909668), ('duration', 2.720458984375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2343.753906), ('duration', 3.955078125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2376.438232), ('duration', 2.42333984375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2522.852783), ('duration', 2.08056640625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2621.444824), ('duration', 2.2861328125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2704.782715), ('duration', 5.006591796875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2779.488281), ('duration', 3.4521484375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2890.36084), ('duration', 4.572509765625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3018.792969), ('duration', 2.2861328125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3073.694824), ('duration', 8.207275390625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3260.755127), ('duration', 1.966064453125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3437.777588), ('duration', 2.468994140625), ('description', 'bad'), ('orig_time', None)])]\n","-------------------------------------\n","N train: 268  N val: 9  N test: 9\n","Opening raw data file ../outputs/rochester_data/natural_speech/after_ica_raw/subj_10_after_ica_raw.fif...\n","    Range : 0 ... 464571 =      0.000 ...  3629.461 secs\n","Ready.\n","Reading 0 ... 464571  =      0.000 ...  3629.461 secs...\n","Initial num of annots: 69  Num of removed annots: 18  Num of retained annots:  51\n"," New annots: [OrderedDict([('onset', 0.0), ('duration', 0.0), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 149.729568), ('duration', 2.286163330078125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 176.793808), ('duration', 1.5088653564453125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 211.101257), ('duration', 1.3259735107421875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 304.370483), ('duration', 2.6748046875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 357.593964), ('duration', 3.429229736328125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 423.59317), ('duration', 4.526611328125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 492.564392), ('duration', 6.309814453125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 537.557739), ('duration', 1.6231689453125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 572.701599), ('duration', 2.994873046875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 612.724426), ('duration', 2.5833740234375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 719.066589), ('duration', 4.8466796875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 811.009827), ('duration', 2.1260986328125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 880.173401), ('duration', 2.42333984375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 887.100464), ('duration', 2.21759033203125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 900.03125), ('duration', 0.0), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1025.614502), ('duration', 2.81201171875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1101.146973), ('duration', 3.8636474609375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1150.301147), ('duration', 2.14892578125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1233.59314), ('duration', 1.7603759765625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1267.456787), ('duration', 3.0406494140625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1362.038574), ('duration', 3.6578369140625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1437.113892), ('duration', 3.4520263671875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1486.748047), ('duration', 2.3319091796875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1505.262085), ('duration', 3.4063720703125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1614.439087), ('duration', 1.0516357421875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1756.245117), ('duration', 4.2523193359375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1807.182495), ('duration', 1.828857421875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1922.290039), ('duration', 2.1490478515625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1982.267212), ('duration', 5.3724365234375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2112.381592), ('duration', 7.91015625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2162.427246), ('duration', 3.93212890625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2216.300293), ('duration', 7.27001953125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2331.169922), ('duration', 2.560546875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2344.951416), ('duration', 2.69775390625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2499.67041), ('duration', 1.486083984375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2523.835205), ('duration', 3.2919921875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2550.506836), ('duration', 3.54345703125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2649.912354), ('duration', 2.994873046875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2706.953857), ('duration', 4.09228515625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2893.044434), ('duration', 3.0634765625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2913.181641), ('duration', 3.360595703125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2961.087891), ('duration', 3.223388671875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2998.279785), ('duration', 3.360595703125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3080.013428), ('duration', 6.88134765625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3253.579834), ('duration', 17.740478515625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3347.544434), ('duration', 2.62890625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3420.438232), ('duration', 2.53759765625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3438.906494), ('duration', 3.109130859375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3501.901367), ('duration', 2.03466796875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3572.518799), ('duration', 1.828857421875), ('description', 'bad'), ('orig_time', None)])]\n","-------------------------------------\n","N train: 315  N val: 10  N test: 10\n","Shape Trian: torch.Size([11558, 1, 129, 640])  Shape Val: torch.Size([374, 1, 129, 640])  Shape Test: torch.Size([360, 1, 129, 640])\n","-------------------------------------\n","Shape EEG Train: torch.Size([11558, 1, 128, 640])  Val: torch.Size([374, 1, 128, 640])  Test: torch.Size([360, 1, 128, 640])\n","Mean: 4.717854579228131e-11  Std: 5.596546088781906e-06\n","Shape Env Train: torch.Size([11558, 1, 1, 640])  Val: torch.Size([374, 1, 1, 640])  Test: torch.Size([360, 1, 1, 640])\n","Mean Env: 2.3700499534606934  Std Env: 2.6003262996673584\n"]}],"source":["raws_train_windowed, raws_val_windowed, raws_test_windowed = [], [], []\n","\n","for subj_id in subj_ids:\n","    \n","\n","    # load subject raw MNE object\n","    raw = mne.io.read_raw(os.path.join(after_ica_path, f'subj_{subj_id}_after_ica_raw.fif'), preload=True)\n","    # drop M1 and M2 channels\n","    raw.drop_channels(['M1', 'M2'])\n","    assert raw.info['nchan'] == n_channs\n","\n","    raw = rm_repeated_annotations(raw)\n","    annots = raw.annotations.copy()\n","    raw_split = [raw.copy().crop(t1, t2) for t1, t2 in zip(annots.onset[:-1]+annots.duration[:-1], annots.onset[1:])]\n","\n","    # Pick the split with the longest duration for validation, supposedly less noisy\n","    ix_val = np.argmax([i.get_data().shape[1] for i in raw_split])\n","    raw_val = [raw_split.pop(ix_val)] # create a list to make it iterable. later may be used for multiple splits\n","\n","    # Pick the next split with the longest duration for testing, supposedly less noisy\n","    ix_test = np.argmax([i.get_data().shape[1] for i in raw_split])\n","    raw_test = [raw_split.pop(ix_test)]\n","    \n","    # creat list of unfolded tensor raw objects\n","    fs = raw.info['sfreq']\n","    raws_train_windowed.extend([unfold_raw(i, window_size=window_size, stride=stride_size_train) for i in raw_split if i.get_data().shape[1] > window_size])\n","    raws_val_windowed.extend([unfold_raw(i, window_size=window_size, stride=stride_size_val) for i in raw_val if i.get_data().shape[1] > window_size])\n","    raws_test_windowed.extend([unfold_raw(i, window_size=window_size, stride=stride_size_test) for i in raw_test if i.get_data().shape[1] > window_size])\n","    print(\"-------------------------------------\")\n","    print('N train: %d  N val: %d  N test: %d' % (len(raws_train_windowed), len(raws_val_windowed), len(raws_test_windowed)))\n","\n","# concatenate all in second dimension\n","sigs_train = torch.cat(raws_train_windowed, dim=1).permute(1, 0, 2, 3)\n","sigs_val = torch.cat(raws_val_windowed, dim=1).permute(1, 0, 2, 3)\n","sigs_test = torch.cat(raws_test_windowed, dim=1).permute(1, 0, 2, 3)\n","print(f\"Shape Trian: {sigs_train.shape}  Shape Val: {sigs_val.shape}  Shape Test: {sigs_test.shape}\")\n","\n","eegs_train = sigs_train[:, :, :-1, :]\n","eegs_val = sigs_val[:, :, :-1, :]\n","eegs_test = sigs_test[:, :, :-1, :]\n","print(\"-------------------------------------\")\n","print(f\"Shape EEG Train: {eegs_train.shape}  Val: {eegs_val.shape}  Test: {eegs_test.shape}\")\n","\n","# To avoid information leakage, we estimate the mean and std from the training set only.\n","mean_eeg_train =  eegs_train.mean()\n","std_eeg_train = eegs_train.std()\n","print(f\"Mean: {mean_eeg_train}  Std: {std_eeg_train}\")\n","\n","envs_train = sigs_train[:, :, [-1], :]\n","envs_val = sigs_val[:, :, [-1], :]\n","envs_test = sigs_test[:, :, [-1], :]\n","print(f\"Shape Env Train: {envs_train.shape}  Val: {envs_val.shape}  Test: {envs_test.shape}\")\n","\n","# Estimate mean and std of the Envelope data set\n","mean_env_train =  envs_train.mean()\n","std_env_train = envs_train.std()\n","print(f\"Mean Env: {mean_env_train}  Std Env: {std_env_train}\")\n","\n","# Normalize the data\n","eegs_train = (eegs_train - mean_eeg_train) / std_eeg_train\n","eegs_val = (eegs_val - mean_eeg_train) / std_eeg_train\n","eegs_test = (eegs_test - mean_eeg_train) / std_eeg_train\n","\n","envs_train = (envs_train - mean_env_train) / std_env_train\n","envs_val = (envs_val - mean_env_train) / std_env_train\n","envs_test = (envs_test - mean_env_train) / std_env_train\n","\n"]},{"cell_type":"markdown","metadata":{"id":"tesRTfOdZQe2"},"source":["### Pytorch dataloader"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1677745431823,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"HviGOmH1ZQe2"},"outputs":[],"source":["class MyDataset(Dataset):\n","    def __init__(self, eeg, env):\n","        self.eeg = eeg\n","        self.env = env\n","    \n","    def __getitem__(self, index):\n","        return self.eeg[index], self.env[index]\n","    \n","    def __len__(self):\n","        return len(self.eeg)\n","    \n","dataset_train = MyDataset(eegs_train, envs_train)\n","dataloader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, drop_last=True)\n","\n","dl_val = DataLoader(MyDataset(eegs_val, envs_val), batch_size=batch_size, shuffle=True, drop_last=True)"]},{"cell_type":"markdown","metadata":{"id":"XMjI2NeFZQe3"},"source":["## Model"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1677745431824,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"57o2oV6VZQe3"},"outputs":[],"source":["class Conv2d(nn.Conv2d):\n","    def __init__(self, in_channels, out_channels, kernel_size, **kargs):\n","        super().__init__(in_channels, out_channels, kernel_size, **kargs)\n","\n","    def __call__(self, inp):\n","        self.out = super().__call__(inp)\n","\n","        if self.out.requires_grad:\n","            self.out.retain_grad()\n","\n","        return self.out\n","    \n","    # -----------------------------------------------------------------------------------------------\n","class Flatten:\n","    \n","  def __call__(self, x):\n","    self.out = x.view(x.shape[0], -1)\n","    return self.out\n","  \n","  def parameters(self):\n","    return []\n","  \n","  # -----------------------------------------------------------------------------------------------\n","class Linear(nn.Linear):\n","    def __init__(self, x, y, **kargs):\n","        super().__init__(x, y, **kargs)\n","\n","    def __call__(self, inp):\n","        self.out = super().__call__(inp)\n","        return self.out\n","  # -----------------------------------------------------------------------------------------------\n","   \n","class ELU(nn.ELU):\n","    def __init__(self, alpha=1.0, inplace=False):\n","        super().__init__(alpha=1.0, inplace=False)\n","\n","    def __call__(self, inp):\n","        self.out = super().__call__(inp)\n","        if self.out.requires_grad:\n","            self.out.retain_grad()\n","        return self.out\n","\n","  # -----------------------------------------------------------------------------------------------\n","class Sequential:\n","  \n","    def __init__(self, layers):\n","        self.layers = layers\n","\n","    def __call__(self, x):\n","        for layer in self.layers:\n","            x = layer(x)\n","        self.out = x\n","        return self.out\n","\n","    def parameters(self):\n","        # get parameters of all layers and stretch them out into one list\n","        return [p for layer in self.layers for p in layer.parameters()]\n","\n","    def named_parameters(self):\n","        # get parameters of all layers and stretch them out into one list\n","        return ((n, p) for layer in self.layers for n, p in layer.named_parameters())"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1677745431824,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"IffWKmD6ZQe3"},"outputs":[],"source":["# My implementation of the shallow convnet\n","\n","fs = 64 # sampling rate\n","T = 5 * fs # number of time points in each trial\n","C = 64 # number of EEG channels\n","F1 = 8 # number of channels (depth) in the first conv layer\n","D = 2 # number of spatial filters in the second conv layer\n","F2 = D * F1 # number of channels (depth) in the pont-wise conv layer\n","num_classes = 4 # number of classes\n","\n","shallow_covnet = Sequential([\n","    Conv2d(1, 40, (1, int(fs//2)), padding='same', bias=True),\n","    Conv2d(40, 40, (C, 1), padding=(0, 0), bias=False), nn.BatchNorm2d(40, affine=True), \n","    nn.AvgPool2d((1, 75), (1, 15)), nn.Dropout(0.5),\n","    Conv2d(40, 4, kernel_size=(1, 30), padding='same', stride=(1, 1), bias=True),\n","    nn.Flatten(1, -1), # Flatten start_dim=1, end_dim=-1\n","    Linear(62*4, 4, bias=True),\n","])\n","\n"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1412,"status":"ok","timestamp":1677745433229,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"bT-TwE-oTAHE","outputId":"e773ece8-5bc9-43a4-89e3-d039a906d189"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([32, 32])\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py:459: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:895.)\n","  return F.conv2d(input, weight, bias, self.stride,\n"]}],"source":["## EEG Encoder with LINEAR\n","\n","class EEGEncoderWithLinear(nn.Module):\n","    def __init__(self,             \n","            fs = 128, # sampling rate\n","            T = 5, # lenght of each trial in seconds\n","            C = 128, # number of EEG channels\n","            F1 = 8, # 8 or 4 number of channels (depth) in the first conv layer\n","            D = 2, # number of spatial filters in the second conv layer\n","            F2 = None # number of channels (depth) in the pont-wise conv layer\n","        ):\n","        super(EEGEncoderWithLinear, self).__init__()\n","\n","        if F2 is None:\n","            F2 = D * F1\n","\n","        self.eeg_encoder = nn.Sequential(\n","            Conv2d(1, F1, (1, int(fs/2)), padding='same', bias=True, groups=1),\n","            nn.BatchNorm2d(F1, affine=True),\n","            Conv2d(F1, out_channels=D*F1, kernel_size=(C, 1), padding=(0, 0), bias=False, groups=F1),\n","            nn.BatchNorm2d(D*F1, affine=True), ELU(), nn.AvgPool2d(1, 4), nn.Dropout(0.25),\n","                    \n","            Conv2d(F2, F2, (1, int(fs/(2*4))), padding='same', bias=False, groups=D*F1),\n","            Conv2d(D*F1, F2, kernel_size=(1, 1), padding=(0, 0), groups=1, bias=False),\n","            nn.BatchNorm2d(F2, affine=True), ELU(), nn.AvgPool2d(1, 8), nn.Dropout(0.25),\n","\n","            nn.Flatten(),\n","            nn.Linear(F2*int((T*fs)//(8*4)), int(fs/4))\n","        ) \n","\n","    def forward(self, x):\n","        x = self.eeg_encoder(x)\n","        return x\n","\n","\n","def normalize_weights_eegnet(eeg_encoder):\n","\n","    for ix, (name, param) in enumerate(eeg_encoder.named_parameters()):\n","        if  name == 'weight' and param.ndim==4 and ix==1: # normalize conv weights to max norm 1\n","            param.data = torch.renorm(param.data, 2, 0, maxnorm=1)\n","        elif name == 'weight' and param.ndim==2: # normalize fc weights to max norm 0.25\n","            param.data = torch.renorm(param.data, 2, 0, maxnorm=0.25)\n","\n","\n","eeg_encoder_with_linear = EEGEncoderWithLinear()\n","\n","# Test the model, add no grad\n","with torch.no_grad():\n","    print(eeg_encoder_with_linear(eegs_train[:32, :, :, :]).shape)\n","\n","#summary(eeg_encoder_with_linear, (1, 128, 640))"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1677745433230,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"sQjEWHA4ZQe4","outputId":"fc31b1f8-1c76-4fff-9d91-c9772c4df655"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([32, 320])\n"]}],"source":["## EEG Encoder NO LINEAR\n","\n","class EEGEncoderNoLinear(nn.Module):\n","    def __init__(self,             \n","            fs = 128, # sampling rate\n","            T = 5, # lenght of each trial in seconds\n","            C = 128, # number of EEG channels\n","            F1 = 8, # 8 or 4 number of channels (depth) in the first conv layer\n","            D = 2, # number of spatial filters in the second conv layer\n","            F2 = None # number of channels (depth) in the pont-wise conv layer\n","        ):\n","        super(EEGEncoderNoLinear, self).__init__()\n","\n","        if F2 is None:\n","            F2 = D * F1\n","\n","        self.eeg_encoder = nn.Sequential(\n","            Conv2d(1, F1, (1, int(fs/2)), padding='same', bias=True, groups=1),\n","            nn.BatchNorm2d(F1, affine=True),\n","            Conv2d(F1, out_channels=D*F1, kernel_size=(C, 1), padding=(0, 0), bias=False, groups=F1),\n","            nn.BatchNorm2d(D*F1, affine=True), ELU(), nn.AvgPool2d(1, 4), nn.Dropout(0.25),\n","                    \n","            Conv2d(F2, F2, (1, int(fs/(2*4))), padding='same', bias=False, groups=D*F1),\n","            Conv2d(D*F1, F2, kernel_size=(1, 1), padding=(0, 0), groups=1, bias=False),\n","            nn.BatchNorm2d(F2, affine=True), ELU(), nn.AvgPool2d(1, 8), nn.Dropout(0.25),\n","\n","            nn.Flatten(),\n","            #nn.Linear(F2*int((T*fs)//(8*4)), int(fs/4))\n","        ) \n","\n","    def forward(self, x):\n","        x = self.eeg_encoder(x)\n","        return x\n","\n","\n","def normalize_weights_eegnet(eeg_encoder):\n","\n","    for ix, (name, param) in enumerate(eeg_encoder.named_parameters()):\n","        if  name == 'weight' and param.ndim==4 and ix==1: # normalize conv weights to max norm 1\n","            param.data = torch.renorm(param.data, 2, 0, maxnorm=1)\n","        elif name == 'weight' and param.ndim==2: # normalize fc weights to max norm 0.25\n","            param.data = torch.renorm(param.data, 2, 0, maxnorm=0.25)\n","\n","\n","eeg_encoder_no_linear = EEGEncoderNoLinear()\n","\n","# Test the model, add no grad\n","with torch.no_grad():\n","    print(eeg_encoder_no_linear(eegs_train[:32, :, :, :]).shape)\n","\n","#summary(eeg_encoder_no_linear, (1, 128, 640))"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1677745433230,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"SfO-UwlzZQe4","outputId":"386ca6b1-c407-42fe-a82d-128d6a293375"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([32, 320])\n"]}],"source":["class EnvEncoder3ConvNoLinear(nn.Module):\n","\n","    def __init__(self,             \n","            fs = 128, # sampling rate\n","            T = 5, # lenght of each trial in seconds\n","            F1 = 4\n","        ):\n","        super(EnvEncoder3ConvNoLinear, self).__init__()\n","\n","        self.env_encoder = nn.Sequential(\n","            Conv2d(1, F1, (1, int(fs//2)), padding='same', bias=True),\n","            nn.BatchNorm2d(F1, affine=True), ELU(), nn.AvgPool2d(1, 2), nn.Dropout(0.5),\n","            Conv2d(F1, F1, (1, int(fs//4)), padding='same', bias=False, groups=1),\n","            nn.BatchNorm2d(F1, affine=True), ELU(), nn.AvgPool2d(1, 2), nn.Dropout(0.5),\n","            Conv2d(F1, F1*4, (1, int(fs//8)), padding='same', bias=False, groups=1),\n","            nn.BatchNorm2d(F1*4, affine=True), ELU(), nn.AvgPool2d(1, 8), nn.Dropout(0.5),\n","            nn.Flatten(),\n","            #nn.Linear(F1*int((T*fs)//(2*8)), int(fs/4))\n","        ) \n","\n","    def forward(self, x):\n","        x = self.env_encoder(x)\n","        return x\n","\n","env_encoder3conv_no_linear = EnvEncoder3ConvNoLinear()\n","\n","\n","# Test the model, add no grad\n","with torch.no_grad():\n","    print(env_encoder3conv_no_linear(envs_train[:32, :, :, :]).shape)\n","#summary(env_encoder3conv_no_linear, (1, 1, 640))"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":485,"status":"ok","timestamp":1677745433712,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"EHbSd3XITAHF","outputId":"3c57a9bc-6c13-4ce4-8314-de9dc049502b"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([32, 320])\n"]}],"source":["class EnvEncoder2ConvNoLinear(nn.Module):\n","\n","    def __init__(self,             \n","            fs = 128, # sampling rate\n","            T = 5, # lenght of each trial in seconds\n","            F1 = 4\n","        ):\n","        super(EnvEncoder2ConvNoLinear, self).__init__()\n","\n","        self.env_encoder = nn.Sequential(\n","            Conv2d(1, F1, (1, int(fs//2)), padding='same', bias=True),\n","            nn.BatchNorm2d(F1, affine=True), ELU(), nn.AvgPool2d(1, 2), nn.Dropout(0.5),\n","            Conv2d(F1, F1*4, (1, int(fs//4)), padding='same', bias=False, groups=1),\n","            nn.BatchNorm2d(F1*4, affine=True), ELU(), nn.AvgPool2d(1, 16), nn.Dropout(0.5),\n","            nn.Flatten(),\n","            #nn.Linear(F1*int((T*fs)//(2*8)), int(fs/4))\n","        ) \n","\n","    def forward(self, x):\n","        x = self.env_encoder(x)\n","        return x\n","\n","env_encoder2conv_no_linear = EnvEncoder2ConvNoLinear()\n","\n","\n","# Test the model, add no grad\n","with torch.no_grad():\n","    print(env_encoder2conv_no_linear(envs_train[:32, :, :, :]).shape)\n","#summary(env_encoder2conv_no_linear, (1, 1, 640))"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1677745433713,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"2914ppmhTAHG","outputId":"9fbe95b7-0663-4d14-acd1-946b7a142c73"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([32, 32])\n"]}],"source":["class EnvEncoder2ConvWithLinear(nn.Module):\n","\n","    def __init__(self,             \n","            fs = 128, # sampling rate\n","            T = 5, # lenght of each trial in seconds\n","            F1 = 4\n","        ):\n","        super(EnvEncoder2ConvWithLinear, self).__init__()\n","\n","        self.env_encoder = nn.Sequential(\n","            Conv2d(1, F1, (1, int(fs//2)), padding='same', bias=True),\n","            nn.BatchNorm2d(F1, affine=True), ELU(), nn.AvgPool2d(1, 2), nn.Dropout(0.5),\n","            Conv2d(F1, F1*4, (1, int(fs//4)), padding='same', bias=False, groups=1),\n","            nn.BatchNorm2d(F1*4, affine=True), ELU(), nn.AvgPool2d(1, 16), nn.Dropout(0.5),\n","            nn.Flatten(),\n","            nn.Linear(F1*4*int((T*fs)//(8*4)), int(fs/4))\n","        ) \n","\n","    def forward(self, x):\n","        x = self.env_encoder(x)\n","        return x\n","\n","env_encoder2conv_with_linear = EnvEncoder2ConvWithLinear()\n","\n","\n","# Test the model, add no grad\n","with torch.no_grad():\n","    print(env_encoder2conv_with_linear(envs_train[:32, :, :, :]).shape)\n","#summary(env_encoder2conv_with_linear, (1, 1, 640))"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1677745433713,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"ojZIx-6eTAHG","outputId":"fd0d8efa-9a11-4629-95be-6b4b227d3b0f"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([32, 32])\n"]}],"source":["class EnvEncoder3ConvWithLinear(nn.Module):\n","\n","    def __init__(self,             \n","            fs = 128, # sampling rate\n","            T = 5, # lenght of each trial in seconds\n","            F1 = 4\n","        ):\n","        super(EnvEncoder3ConvWithLinear, self).__init__()\n","\n","        self.env_encoder = nn.Sequential(\n","            Conv2d(1, F1, (1, int(fs//2)), padding='same', bias=True),\n","            nn.BatchNorm2d(F1, affine=True), ELU(), nn.AvgPool2d(1, 2), nn.Dropout(0.5),\n","            Conv2d(F1, F1, (1, int(fs//4)), padding='same', bias=False, groups=1),\n","            nn.BatchNorm2d(F1, affine=True), ELU(), nn.AvgPool2d(1, 2), nn.Dropout(0.5),\n","            Conv2d(F1, F1*4, (1, int(fs//8)), padding='same', bias=False, groups=1),\n","            nn.BatchNorm2d(F1*4, affine=True), ELU(), nn.AvgPool2d(1, 8), nn.Dropout(0.5),\n","            nn.Flatten(),\n","            nn.Linear(F1*4*int((T*fs)//(4*8)), int(fs/4))\n","        ) \n","\n","    def forward(self, x):\n","        x = self.env_encoder(x)\n","        return x\n","\n","env_encoder3conv_with_linear = EnvEncoder3ConvWithLinear()\n","\n","\n","# Test the model, add no grad\n","with torch.no_grad():\n","    print(env_encoder3conv_with_linear(envs_train[:32, :, :, :]).shape)\n","#summary(env_encoder3conv_with_linear, (1, 1, 640))"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2485,"status":"ok","timestamp":1677745436193,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"Bp8HVS-aZQe4","outputId":"03ab169d-a4c2-45d2-caa2-706838006169"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["CES()"]},"metadata":{},"execution_count":20}],"source":["class CES(nn.Module):\n","    def __init__(self, \n","                 eeg_encoder= None,\n","                 env_encoder = None): \n","        super().__init__()\n","\n","        self.eeg_encoder = eeg_encoder\n","        self.env_encoder = env_encoder\n","        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n","\n","    def encode_eeg(self, x):\n","        return self.eeg_encoder(x)\n","    \n","    def encode_env(self, x):\n","        return self.env_encoder(x)\n","    \n","    def forward(self, eeg, env):\n","        eeg_features = self.encode_eeg(eeg)\n","        env_features = self.encode_env(env)\n","        return eeg_features, env_features, self.logit_scale.exp()\n","  \n","\n","model = CES();\n","model.to(device)\n","#for n,p in model.named_parameters():\n","    #print(n, p.shape)\n"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":188,"status":"ok","timestamp":1677745454889,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"kRK-Pq1VTAHG","outputId":"f5333ff8-df5b-48ef-87f4-c57aca9cf9e8"},"outputs":[{"output_type":"stream","name":"stdout","text":[" Models with no Linear Layers\n"," Models with Linear Layers\n"]}],"source":["print(\" Models with no Linear Layers\")\n","eeg_encoder_no_linear = EEGEncoderNoLinear()\n","env_encoder3conv_no_linear = EnvEncoder2ConvNoLinear()\n","ces_eeg_0lin_env_3conv_0lin = CES(eeg_encoder=eeg_encoder_no_linear.eeg_encoder, env_encoder=env_encoder2conv_no_linear.env_encoder)\n","#summary(ces_eeg_0lin_env_3conv_0lin, [(1, 128, 640), (1, 1, 640)])\n","\n","eeg_encoder_no_linear = EEGEncoderNoLinear()\n","env_encoder2conv_no_linear = EnvEncoder2ConvNoLinear()\n","ces_eeg_0lin_env_2conv_0lin = CES(eeg_encoder=eeg_encoder_no_linear.eeg_encoder, env_encoder=env_encoder2conv_no_linear.env_encoder)\n","#summary(ces_eeg_0lin_env_2conv_0lin, [(1, 128, 640), (1, 1, 640)])\n","\n","\n","print(\" Models with Linear Layers\")\n","eeg_encoder_with_linear = EEGEncoderWithLinear()\n","env_encoder3conv_with_linear = EnvEncoder2ConvWithLinear()\n","ces_eeg_1lin_env_3conv_1lin = CES(eeg_encoder=eeg_encoder_with_linear.eeg_encoder, env_encoder=env_encoder3conv_with_linear.env_encoder)\n","#summary(ces_eeg_1lin_env_3conv_1lin, [(1, 128, 640), (1, 1, 640)])\n","\n","eeg_encoder_with_linear = EEGEncoderWithLinear()\n","env_encoder2conv_with_linear = EnvEncoder2ConvWithLinear()\n","ces_eeg_1lin_env_2conv_1lin = CES(eeg_encoder=eeg_encoder_with_linear.eeg_encoder, env_encoder=env_encoder2conv_with_linear.env_encoder)\n","#summary(ces_eeg_1lin_env_2conv_1lin, [(1, 128, 640), (1, 1, 640)])\n","\n","models_name = [\"eeg0lin_env3conv0lin\", \"eeg0lin_env2conv0lin\"]#, \"eeg1lin_env3conv1lin\", \"eeg1lin_env2conv1lin\"]\n","models_dict = {\"eeg0lin_env3conv0lin\": ces_eeg_0lin_env_3conv_0lin, \"eeg0lin_env2conv0lin\": ces_eeg_0lin_env_2conv_0lin} #, \n","              # \"eeg1lin_env3conv1lin\": ces_eeg_1lin_env_3conv_1lin, \"eeg1lin_env2conv1lin\": ces_eeg_1lin_env_2conv_1lin}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"d4iJiosNZQe7","outputId":"39e72cf7-9ed7-4149-8030-5fd49f14f7f8"},"outputs":[{"output_type":"stream","name":"stdout","text":["+--------------New model: eeg0lin_env3conv0lin----------------------+\n"]},{"output_type":"display_data","data":{"text/plain":["Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["NumExpr defaulting to 8 threads.\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">NumExpr defaulting to 8 threads.\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["====== Epoch: 1\n","====> Validation loss: 3.2509,  X1 loss: 3.2421   X2 loss: 3.2596\n","====== Epoch: 2\n","====> Validation loss: 3.0907,  X1 loss: 3.0881   X2 loss: 3.0933\n","====== Epoch: 3\n","====> Validation loss: 3.0363,  X1 loss: 3.0381   X2 loss: 3.0344\n","====== Epoch: 4\n","====> Validation loss: 3.0065,  X1 loss: 3.0040   X2 loss: 3.0090\n","====== Epoch: 5\n","====> Validation loss: 2.9992,  X1 loss: 2.9991   X2 loss: 2.9994\n","====== Epoch: 6\n","====> Validation loss: 2.9618,  X1 loss: 2.9565   X2 loss: 2.9671\n","====== Epoch: 7\n","====> Validation loss: 2.9230,  X1 loss: 2.9209   X2 loss: 2.9250\n","====== Epoch: 8\n","====> Validation loss: 2.9720,  X1 loss: 2.9641   X2 loss: 2.9800\n","====== Epoch: 9\n","====> Validation loss: 2.8835,  X1 loss: 2.8712   X2 loss: 2.8959\n","====== Epoch: 10\n","====> Validation loss: 2.9201,  X1 loss: 2.9182   X2 loss: 2.9220\n","====== Epoch: 11\n","====> Validation loss: 2.9127,  X1 loss: 2.8985   X2 loss: 2.9269\n","====== Epoch: 12\n","====> Validation loss: 2.9270,  X1 loss: 2.9211   X2 loss: 2.9328\n","====== Epoch: 13\n","====> Validation loss: 2.8502,  X1 loss: 2.8297   X2 loss: 2.8707\n","====== Epoch: 14\n","====> Validation loss: 2.9565,  X1 loss: 2.9422   X2 loss: 2.9708\n","====== Epoch: 15\n","====> Validation loss: 2.9546,  X1 loss: 2.9374   X2 loss: 2.9717\n","====== Epoch: 16\n","====> Validation loss: 2.8780,  X1 loss: 2.8704   X2 loss: 2.8856\n","====== Epoch: 17\n","====> Validation loss: 2.8224,  X1 loss: 2.8142   X2 loss: 2.8306\n","====== Epoch: 18\n","====> Validation loss: 2.8544,  X1 loss: 2.8542   X2 loss: 2.8545\n","====== Epoch: 19\n","====> Validation loss: 2.8590,  X1 loss: 2.8406   X2 loss: 2.8775\n","====== Epoch: 20\n","====> Validation loss: 2.8713,  X1 loss: 2.8631   X2 loss: 2.8795\n","====== Epoch: 21\n","====> Validation loss: 2.8500,  X1 loss: 2.8331   X2 loss: 2.8669\n","====== Epoch: 22\n","====> Validation loss: 2.8387,  X1 loss: 2.8336   X2 loss: 2.8438\n","====== Epoch: 23\n","====> Validation loss: 2.7666,  X1 loss: 2.7480   X2 loss: 2.7852\n","====== Epoch: 24\n","====> Validation loss: 2.8549,  X1 loss: 2.8432   X2 loss: 2.8666\n","====== Epoch: 25\n","====> Validation loss: 2.9309,  X1 loss: 2.9213   X2 loss: 2.9405\n","====== Epoch: 26\n","====> Validation loss: 2.8468,  X1 loss: 2.8390   X2 loss: 2.8546\n","====== Epoch: 27\n","====> Validation loss: 2.8775,  X1 loss: 2.8593   X2 loss: 2.8958\n","====== Epoch: 28\n","====> Validation loss: 2.8445,  X1 loss: 2.8365   X2 loss: 2.8525\n","====== Epoch: 29\n","====> Validation loss: 2.8465,  X1 loss: 2.8379   X2 loss: 2.8552\n","====== Epoch: 30\n","====> Validation loss: 2.8364,  X1 loss: 2.8260   X2 loss: 2.8469\n","====== Epoch: 31\n","====> Validation loss: 2.8569,  X1 loss: 2.8425   X2 loss: 2.8713\n","====== Epoch: 32\n","====> Validation loss: 2.9353,  X1 loss: 2.9385   X2 loss: 2.9321\n","====== Epoch: 33\n","====> Validation loss: 2.8884,  X1 loss: 2.8893   X2 loss: 2.8875\n","====== Epoch: 34\n","====> Validation loss: 2.7600,  X1 loss: 2.7496   X2 loss: 2.7705\n","====== Epoch: 35\n","====> Validation loss: 2.8474,  X1 loss: 2.8497   X2 loss: 2.8452\n","====== Epoch: 36\n","====> Validation loss: 2.8359,  X1 loss: 2.8220   X2 loss: 2.8498\n","====== Epoch: 37\n","====> Validation loss: 2.8423,  X1 loss: 2.8345   X2 loss: 2.8501\n","====== Epoch: 38\n","====> Validation loss: 2.8485,  X1 loss: 2.8359   X2 loss: 2.8611\n","====== Epoch: 39\n","====> Validation loss: 2.8334,  X1 loss: 2.8216   X2 loss: 2.8451\n","====== Epoch: 40\n","====> Validation loss: 2.8686,  X1 loss: 2.8572   X2 loss: 2.8801\n","====== Epoch: 41\n","====> Validation loss: 2.8828,  X1 loss: 2.8679   X2 loss: 2.8977\n","====== Epoch: 42\n","====> Validation loss: 2.8194,  X1 loss: 2.8009   X2 loss: 2.8378\n","====== Epoch: 43\n","====> Validation loss: 2.8200,  X1 loss: 2.7990   X2 loss: 2.8410\n","====== Epoch: 44\n","====> Validation loss: 2.8262,  X1 loss: 2.8092   X2 loss: 2.8433\n","====== Epoch: 45\n","====> Validation loss: 2.7898,  X1 loss: 2.7786   X2 loss: 2.8009\n","====== Epoch: 46\n","====> Validation loss: 2.8335,  X1 loss: 2.8253   X2 loss: 2.8416\n","====== Epoch: 47\n","====> Validation loss: 2.8287,  X1 loss: 2.8219   X2 loss: 2.8355\n","====== Epoch: 48\n","====> Validation loss: 2.8430,  X1 loss: 2.8321   X2 loss: 2.8538\n","====== Epoch: 49\n","====> Validation loss: 2.7501,  X1 loss: 2.7289   X2 loss: 2.7712\n","====== Epoch: 50\n","====> Validation loss: 2.8282,  X1 loss: 2.8263   X2 loss: 2.8300\n","====== Epoch: 51\n","====> Validation loss: 2.8499,  X1 loss: 2.8213   X2 loss: 2.8784\n","====== Epoch: 52\n","====> Validation loss: 2.8067,  X1 loss: 2.8037   X2 loss: 2.8098\n","====== Epoch: 53\n","====> Validation loss: 2.8527,  X1 loss: 2.8376   X2 loss: 2.8679\n","====== Epoch: 54\n","====> Validation loss: 2.7836,  X1 loss: 2.7806   X2 loss: 2.7866\n","====== Epoch: 55\n","====> Validation loss: 2.7735,  X1 loss: 2.7710   X2 loss: 2.7759\n","====== Epoch: 56\n","====> Validation loss: 2.8038,  X1 loss: 2.7887   X2 loss: 2.8190\n","====== Epoch: 57\n","====> Validation loss: 2.9151,  X1 loss: 2.9022   X2 loss: 2.9281\n","====== Epoch: 58\n","====> Validation loss: 2.8575,  X1 loss: 2.8558   X2 loss: 2.8593\n","====== Epoch: 59\n","====> Validation loss: 2.8022,  X1 loss: 2.7839   X2 loss: 2.8206\n","====== Epoch: 60\n","====> Validation loss: 2.8128,  X1 loss: 2.8023   X2 loss: 2.8232\n","====== Epoch: 61\n","====> Validation loss: 2.8085,  X1 loss: 2.8025   X2 loss: 2.8145\n","====== Epoch: 62\n","====> Validation loss: 2.8132,  X1 loss: 2.7998   X2 loss: 2.8266\n","====== Epoch: 63\n","====> Validation loss: 2.7875,  X1 loss: 2.7830   X2 loss: 2.7919\n","====== Epoch: 64\n","====> Validation loss: 2.8152,  X1 loss: 2.7997   X2 loss: 2.8307\n","====== Epoch: 65\n","====> Validation loss: 2.7609,  X1 loss: 2.7518   X2 loss: 2.7700\n","====== Epoch: 66\n","====> Validation loss: 2.7581,  X1 loss: 2.7533   X2 loss: 2.7629\n","====== Epoch: 67\n","====> Validation loss: 2.7948,  X1 loss: 2.7766   X2 loss: 2.8130\n","====== Epoch: 68\n","====> Validation loss: 2.7678,  X1 loss: 2.7643   X2 loss: 2.7712\n","====== Epoch: 69\n","====> Validation loss: 2.8391,  X1 loss: 2.8323   X2 loss: 2.8460\n","====== Epoch: 70\n","====> Validation loss: 2.7881,  X1 loss: 2.7788   X2 loss: 2.7973\n","====== Epoch: 71\n","====> Validation loss: 2.7403,  X1 loss: 2.7294   X2 loss: 2.7512\n","====== Epoch: 72\n","====> Validation loss: 2.8143,  X1 loss: 2.8059   X2 loss: 2.8227\n","====== Epoch: 73\n","====> Validation loss: 2.7755,  X1 loss: 2.7631   X2 loss: 2.7878\n","====== Epoch: 74\n","====> Validation loss: 2.8172,  X1 loss: 2.8156   X2 loss: 2.8188\n","====== Epoch: 75\n","====> Validation loss: 2.8075,  X1 loss: 2.7890   X2 loss: 2.8259\n","====== Epoch: 76\n","====> Validation loss: 2.8501,  X1 loss: 2.8380   X2 loss: 2.8622\n","====== Epoch: 77\n","====> Validation loss: 2.8393,  X1 loss: 2.8321   X2 loss: 2.8465\n","====== Epoch: 78\n","====> Validation loss: 2.8222,  X1 loss: 2.8289   X2 loss: 2.8155\n","====== Epoch: 79\n","====> Validation loss: 2.8778,  X1 loss: 2.8726   X2 loss: 2.8830\n","====== Epoch: 80\n","====> Validation loss: 2.8200,  X1 loss: 2.7955   X2 loss: 2.8444\n","====== Epoch: 81\n","====> Validation loss: 2.8233,  X1 loss: 2.8208   X2 loss: 2.8258\n","====== Epoch: 82\n","====> Validation loss: 2.8398,  X1 loss: 2.8300   X2 loss: 2.8495\n","====== Epoch: 83\n","====> Validation loss: 2.7954,  X1 loss: 2.7836   X2 loss: 2.8073\n","====== Epoch: 84\n","====> Validation loss: 2.8301,  X1 loss: 2.8178   X2 loss: 2.8423\n","====== Epoch: 85\n","====> Validation loss: 2.8362,  X1 loss: 2.8219   X2 loss: 2.8504\n","====== Epoch: 86\n","====> Validation loss: 2.7998,  X1 loss: 2.7990   X2 loss: 2.8007\n","====== Epoch: 87\n","====> Validation loss: 2.8873,  X1 loss: 2.8700   X2 loss: 2.9047\n","====== Epoch: 88\n","====> Validation loss: 2.8141,  X1 loss: 2.8037   X2 loss: 2.8244\n","====== Epoch: 89\n","====> Validation loss: 2.7890,  X1 loss: 2.7837   X2 loss: 2.7942\n","====== Epoch: 90\n","====> Validation loss: 2.8164,  X1 loss: 2.8096   X2 loss: 2.8233\n","====== Epoch: 91\n","====> Validation loss: 2.8643,  X1 loss: 2.8568   X2 loss: 2.8718\n","====== Epoch: 92\n","====> Validation loss: 2.7872,  X1 loss: 2.7733   X2 loss: 2.8011\n","====== Epoch: 93\n","====> Validation loss: 2.8275,  X1 loss: 2.8118   X2 loss: 2.8433\n","====== Epoch: 94\n","====> Validation loss: 2.8644,  X1 loss: 2.8422   X2 loss: 2.8866\n","====== Epoch: 95\n","====> Validation loss: 2.8138,  X1 loss: 2.8021   X2 loss: 2.8255\n","====== Epoch: 96\n","====> Validation loss: 2.8078,  X1 loss: 2.7904   X2 loss: 2.8253\n","====== Epoch: 97\n","====> Validation loss: 2.8162,  X1 loss: 2.8087   X2 loss: 2.8236\n","====== Epoch: 98\n","====> Validation loss: 2.8101,  X1 loss: 2.8035   X2 loss: 2.8167\n","====== Epoch: 99\n","====> Validation loss: 2.8346,  X1 loss: 2.8175   X2 loss: 2.8517\n","====== Epoch: 100\n","====> Validation loss: 2.8244,  X1 loss: 2.8298   X2 loss: 2.8190\n","====== Epoch: 101\n","====> Validation loss: 2.8706,  X1 loss: 2.8525   X2 loss: 2.8886\n","====== Epoch: 102\n","====> Validation loss: 2.8097,  X1 loss: 2.8014   X2 loss: 2.8181\n","====== Epoch: 103\n","====> Validation loss: 2.8562,  X1 loss: 2.8527   X2 loss: 2.8596\n","====== Epoch: 104\n","====> Validation loss: 2.8382,  X1 loss: 2.8188   X2 loss: 2.8576\n","====== Epoch: 105\n","====> Validation loss: 2.8555,  X1 loss: 2.8443   X2 loss: 2.8668\n","====== Epoch: 106\n","====> Validation loss: 2.7780,  X1 loss: 2.7660   X2 loss: 2.7901\n","====== Epoch: 107\n","====> Validation loss: 2.8078,  X1 loss: 2.7807   X2 loss: 2.8349\n","====== Epoch: 108\n","====> Validation loss: 2.8321,  X1 loss: 2.8089   X2 loss: 2.8553\n","====== Epoch: 109\n","====> Validation loss: 2.8683,  X1 loss: 2.8684   X2 loss: 2.8682\n","====== Epoch: 110\n","====> Validation loss: 2.8441,  X1 loss: 2.8379   X2 loss: 2.8503\n","====== Epoch: 111\n","====> Validation loss: 2.7632,  X1 loss: 2.7484   X2 loss: 2.7779\n","====== Epoch: 112\n","====> Validation loss: 2.8346,  X1 loss: 2.8268   X2 loss: 2.8424\n","====== Epoch: 113\n","====> Validation loss: 2.7965,  X1 loss: 2.7895   X2 loss: 2.8034\n","====== Epoch: 114\n","====> Validation loss: 2.7764,  X1 loss: 2.7625   X2 loss: 2.7903\n","====== Epoch: 115\n","====> Validation loss: 2.8065,  X1 loss: 2.8006   X2 loss: 2.8123\n","====== Epoch: 116\n","====> Validation loss: 2.8246,  X1 loss: 2.8098   X2 loss: 2.8393\n","====== Epoch: 117\n","====> Validation loss: 2.7728,  X1 loss: 2.7644   X2 loss: 2.7812\n","====== Epoch: 118\n","====> Validation loss: 2.8540,  X1 loss: 2.8334   X2 loss: 2.8747\n","====== Epoch: 119\n","====> Validation loss: 2.8972,  X1 loss: 2.8807   X2 loss: 2.9137\n","====== Epoch: 120\n","====> Validation loss: 2.8136,  X1 loss: 2.7921   X2 loss: 2.8351\n","====== Epoch: 121\n","====> Validation loss: 2.8048,  X1 loss: 2.7987   X2 loss: 2.8108\n","====== Epoch: 122\n","====> Validation loss: 2.8069,  X1 loss: 2.8009   X2 loss: 2.8129\n","====== Epoch: 123\n","====> Validation loss: 2.7878,  X1 loss: 2.7740   X2 loss: 2.8015\n","====== Epoch: 124\n","====> Validation loss: 2.7951,  X1 loss: 2.7700   X2 loss: 2.8202\n","====== Epoch: 125\n","====> Validation loss: 2.7904,  X1 loss: 2.7802   X2 loss: 2.8005\n","====== Epoch: 126\n","====> Validation loss: 2.7953,  X1 loss: 2.7908   X2 loss: 2.7998\n","====== Epoch: 127\n","====> Validation loss: 2.7991,  X1 loss: 2.7859   X2 loss: 2.8123\n","====== Epoch: 128\n","====> Validation loss: 2.7948,  X1 loss: 2.7901   X2 loss: 2.7996\n","====== Epoch: 129\n","====> Validation loss: 2.9052,  X1 loss: 2.8889   X2 loss: 2.9215\n","====== Epoch: 130\n","====> Validation loss: 2.8061,  X1 loss: 2.7834   X2 loss: 2.8288\n","====== Epoch: 131\n","====> Validation loss: 2.8190,  X1 loss: 2.8071   X2 loss: 2.8310\n","====== Epoch: 132\n","====> Validation loss: 2.8436,  X1 loss: 2.8369   X2 loss: 2.8503\n","====== Epoch: 133\n","====> Validation loss: 2.8653,  X1 loss: 2.8432   X2 loss: 2.8874\n","====== Epoch: 134\n","====> Validation loss: 2.8449,  X1 loss: 2.8229   X2 loss: 2.8670\n","====== Epoch: 135\n","====> Validation loss: 2.8523,  X1 loss: 2.8425   X2 loss: 2.8622\n","====== Epoch: 136\n","====> Validation loss: 2.8886,  X1 loss: 2.8825   X2 loss: 2.8948\n","====== Epoch: 137\n","====> Validation loss: 2.8129,  X1 loss: 2.8024   X2 loss: 2.8233\n","====== Epoch: 138\n","====> Validation loss: 2.8077,  X1 loss: 2.7897   X2 loss: 2.8256\n","====== Epoch: 139\n","====> Validation loss: 2.8497,  X1 loss: 2.8404   X2 loss: 2.8590\n","====== Epoch: 140\n","====> Validation loss: 2.8505,  X1 loss: 2.8407   X2 loss: 2.8602\n","====== Epoch: 141\n","====> Validation loss: 2.8752,  X1 loss: 2.8574   X2 loss: 2.8931\n","====== Epoch: 142\n","====> Validation loss: 2.8352,  X1 loss: 2.8286   X2 loss: 2.8418\n","====== Epoch: 143\n","====> Validation loss: 2.8074,  X1 loss: 2.7945   X2 loss: 2.8204\n","====== Epoch: 144\n","====> Validation loss: 2.8838,  X1 loss: 2.8597   X2 loss: 2.9079\n","====== Epoch: 145\n","====> Validation loss: 2.7729,  X1 loss: 2.7672   X2 loss: 2.7786\n","====== Epoch: 146\n","====> Validation loss: 2.8046,  X1 loss: 2.7916   X2 loss: 2.8175\n","====== Epoch: 147\n","====> Validation loss: 2.8950,  X1 loss: 2.8723   X2 loss: 2.9177\n","====== Epoch: 148\n","====> Validation loss: 2.8177,  X1 loss: 2.8003   X2 loss: 2.8351\n","====== Epoch: 149\n","====> Validation loss: 2.8621,  X1 loss: 2.8435   X2 loss: 2.8807\n","====== Epoch: 150\n","====> Validation loss: 2.8638,  X1 loss: 2.8452   X2 loss: 2.8823\n","====== Epoch: 151\n","====> Validation loss: 2.8163,  X1 loss: 2.8004   X2 loss: 2.8322\n","====== Epoch: 152\n","====> Validation loss: 2.7241,  X1 loss: 2.7187   X2 loss: 2.7295\n","====== Epoch: 153\n","====> Validation loss: 2.8223,  X1 loss: 2.8004   X2 loss: 2.8442\n","====== Epoch: 154\n","====> Validation loss: 2.8726,  X1 loss: 2.8666   X2 loss: 2.8786\n","====== Epoch: 155\n","====> Validation loss: 2.8102,  X1 loss: 2.7914   X2 loss: 2.8290\n","====== Epoch: 156\n","====> Validation loss: 2.8136,  X1 loss: 2.8077   X2 loss: 2.8194\n","====== Epoch: 157\n","====> Validation loss: 2.8144,  X1 loss: 2.7953   X2 loss: 2.8335\n","====== Epoch: 158\n","====> Validation loss: 2.8238,  X1 loss: 2.8032   X2 loss: 2.8444\n","====== Epoch: 159\n","====> Validation loss: 2.8959,  X1 loss: 2.8819   X2 loss: 2.9099\n","====== Epoch: 160\n","====> Validation loss: 2.8060,  X1 loss: 2.7791   X2 loss: 2.8329\n","====== Epoch: 161\n","====> Validation loss: 2.8800,  X1 loss: 2.8665   X2 loss: 2.8934\n","====== Epoch: 162\n","====> Validation loss: 2.8026,  X1 loss: 2.7804   X2 loss: 2.8247\n","====== Epoch: 163\n","====> Validation loss: 2.8547,  X1 loss: 2.8527   X2 loss: 2.8567\n","====== Epoch: 164\n","====> Validation loss: 2.8641,  X1 loss: 2.8490   X2 loss: 2.8792\n","====== Epoch: 165\n","====> Validation loss: 2.8182,  X1 loss: 2.8101   X2 loss: 2.8262\n","====== Epoch: 166\n","====> Validation loss: 2.8669,  X1 loss: 2.8450   X2 loss: 2.8888\n","====== Epoch: 167\n","====> Validation loss: 2.8016,  X1 loss: 2.7905   X2 loss: 2.8128\n","====== Epoch: 168\n","====> Validation loss: 2.8406,  X1 loss: 2.8329   X2 loss: 2.8482\n","====== Epoch: 169\n","====> Validation loss: 2.8192,  X1 loss: 2.8092   X2 loss: 2.8291\n","====== Epoch: 170\n","====> Validation loss: 2.7967,  X1 loss: 2.7937   X2 loss: 2.7997\n","====== Epoch: 171\n","====> Validation loss: 2.8923,  X1 loss: 2.8750   X2 loss: 2.9097\n","====== Epoch: 172\n","====> Validation loss: 2.8105,  X1 loss: 2.8077   X2 loss: 2.8134\n","====== Epoch: 173\n","====> Validation loss: 2.8210,  X1 loss: 2.8175   X2 loss: 2.8245\n","====== Epoch: 174\n","====> Validation loss: 2.8760,  X1 loss: 2.8599   X2 loss: 2.8921\n","====== Epoch: 175\n","====> Validation loss: 2.8200,  X1 loss: 2.8080   X2 loss: 2.8320\n","====== Epoch: 176\n","====> Validation loss: 2.7666,  X1 loss: 2.7607   X2 loss: 2.7726\n","====== Epoch: 177\n","====> Validation loss: 2.8584,  X1 loss: 2.8513   X2 loss: 2.8656\n","====== Epoch: 178\n","====> Validation loss: 2.8301,  X1 loss: 2.8183   X2 loss: 2.8419\n","====== Epoch: 179\n","====> Validation loss: 2.9189,  X1 loss: 2.9102   X2 loss: 2.9276\n","====== Epoch: 180\n","====> Validation loss: 2.8083,  X1 loss: 2.7778   X2 loss: 2.8389\n","====== Epoch: 181\n","====> Validation loss: 2.9307,  X1 loss: 2.9189   X2 loss: 2.9424\n","====== Epoch: 182\n","====> Validation loss: 2.8629,  X1 loss: 2.8432   X2 loss: 2.8825\n","====== Epoch: 183\n","====> Validation loss: 2.8172,  X1 loss: 2.7845   X2 loss: 2.8500\n","====== Epoch: 184\n","====> Validation loss: 2.8010,  X1 loss: 2.7788   X2 loss: 2.8233\n","====== Epoch: 185\n","====> Validation loss: 2.8011,  X1 loss: 2.7843   X2 loss: 2.8179\n","====== Epoch: 186\n","====> Validation loss: 2.8365,  X1 loss: 2.8231   X2 loss: 2.8499\n","====== Epoch: 187\n","====> Validation loss: 2.8525,  X1 loss: 2.8472   X2 loss: 2.8578\n","====== Epoch: 188\n","====> Validation loss: 2.7985,  X1 loss: 2.7738   X2 loss: 2.8233\n","====== Epoch: 189\n","====> Validation loss: 2.8548,  X1 loss: 2.8213   X2 loss: 2.8884\n","====== Epoch: 190\n","====> Validation loss: 2.8372,  X1 loss: 2.8229   X2 loss: 2.8515\n","====== Epoch: 191\n","====> Validation loss: 2.8603,  X1 loss: 2.8457   X2 loss: 2.8750\n","====== Epoch: 192\n","====> Validation loss: 2.7974,  X1 loss: 2.7849   X2 loss: 2.8100\n","====== Epoch: 193\n","====> Validation loss: 2.8078,  X1 loss: 2.8038   X2 loss: 2.8119\n","====== Epoch: 194\n","====> Validation loss: 2.8709,  X1 loss: 2.8536   X2 loss: 2.8882\n","====== Epoch: 195\n","====> Validation loss: 2.7891,  X1 loss: 2.7705   X2 loss: 2.8078\n","====== Epoch: 196\n","====> Validation loss: 2.8002,  X1 loss: 2.7892   X2 loss: 2.8111\n","====== Epoch: 197\n","====> Validation loss: 2.8518,  X1 loss: 2.8425   X2 loss: 2.8611\n","====== Epoch: 198\n","====> Validation loss: 2.8726,  X1 loss: 2.8434   X2 loss: 2.9019\n","====== Epoch: 199\n","====> Validation loss: 2.7800,  X1 loss: 2.7663   X2 loss: 2.7937\n","====== Epoch: 200\n","====> Validation loss: 2.7872,  X1 loss: 2.7769   X2 loss: 2.7976\n","====== Epoch: 201\n","====> Validation loss: 2.8222,  X1 loss: 2.8000   X2 loss: 2.8443\n","====== Epoch: 202\n","====> Validation loss: 2.7975,  X1 loss: 2.7877   X2 loss: 2.8073\n","====== Epoch: 203\n","====> Validation loss: 2.8192,  X1 loss: 2.8142   X2 loss: 2.8242\n","====== Epoch: 204\n","====> Validation loss: 2.7994,  X1 loss: 2.7803   X2 loss: 2.8185\n","====== Epoch: 205\n","====> Validation loss: 2.8581,  X1 loss: 2.8395   X2 loss: 2.8766\n","====== Epoch: 206\n","====> Validation loss: 2.8158,  X1 loss: 2.8011   X2 loss: 2.8305\n","====== Epoch: 207\n","====> Validation loss: 2.8433,  X1 loss: 2.8119   X2 loss: 2.8747\n","====== Epoch: 208\n","====> Validation loss: 2.8069,  X1 loss: 2.7824   X2 loss: 2.8313\n","====== Epoch: 209\n","====> Validation loss: 2.8233,  X1 loss: 2.8173   X2 loss: 2.8294\n","====== Epoch: 210\n","====> Validation loss: 2.8135,  X1 loss: 2.7820   X2 loss: 2.8449\n","====== Epoch: 211\n","====> Validation loss: 2.8526,  X1 loss: 2.8282   X2 loss: 2.8770\n","====== Epoch: 212\n","====> Validation loss: 2.8250,  X1 loss: 2.8010   X2 loss: 2.8490\n","====== Epoch: 213\n","====> Validation loss: 2.8226,  X1 loss: 2.8161   X2 loss: 2.8290\n","====== Epoch: 214\n","====> Validation loss: 2.8389,  X1 loss: 2.8138   X2 loss: 2.8641\n","====== Epoch: 215\n","====> Validation loss: 2.8324,  X1 loss: 2.8131   X2 loss: 2.8517\n","====== Epoch: 216\n","====> Validation loss: 2.7939,  X1 loss: 2.7693   X2 loss: 2.8185\n","====== Epoch: 217\n","====> Validation loss: 2.8083,  X1 loss: 2.7983   X2 loss: 2.8182\n","====== Epoch: 218\n","====> Validation loss: 2.8844,  X1 loss: 2.8639   X2 loss: 2.9048\n","====== Epoch: 219\n","====> Validation loss: 2.8737,  X1 loss: 2.8585   X2 loss: 2.8889\n","====== Epoch: 220\n","====> Validation loss: 2.8125,  X1 loss: 2.7930   X2 loss: 2.8321\n","====== Epoch: 221\n","====> Validation loss: 2.8043,  X1 loss: 2.7915   X2 loss: 2.8172\n","====== Epoch: 222\n","====> Validation loss: 2.8561,  X1 loss: 2.8488   X2 loss: 2.8634\n","====== Epoch: 223\n","====> Validation loss: 2.8228,  X1 loss: 2.8176   X2 loss: 2.8280\n","====== Epoch: 224\n","====> Validation loss: 2.8142,  X1 loss: 2.8096   X2 loss: 2.8188\n","====== Epoch: 225\n","====> Validation loss: 2.8466,  X1 loss: 2.8203   X2 loss: 2.8730\n","====== Epoch: 226\n","====> Validation loss: 2.8459,  X1 loss: 2.8283   X2 loss: 2.8636\n","====== Epoch: 227\n","====> Validation loss: 2.9116,  X1 loss: 2.8967   X2 loss: 2.9266\n","====== Epoch: 228\n","====> Validation loss: 2.8342,  X1 loss: 2.8232   X2 loss: 2.8451\n","====== Epoch: 229\n","====> Validation loss: 2.7972,  X1 loss: 2.7749   X2 loss: 2.8195\n","====== Epoch: 230\n","====> Validation loss: 2.9056,  X1 loss: 2.8857   X2 loss: 2.9255\n","====== Epoch: 231\n","====> Validation loss: 2.8275,  X1 loss: 2.8243   X2 loss: 2.8306\n","====== Epoch: 232\n","====> Validation loss: 2.7826,  X1 loss: 2.7494   X2 loss: 2.8158\n","====== Epoch: 233\n","====> Validation loss: 2.8323,  X1 loss: 2.8017   X2 loss: 2.8629\n","====== Epoch: 234\n","====> Validation loss: 2.8801,  X1 loss: 2.8704   X2 loss: 2.8898\n","====== Epoch: 235\n","====> Validation loss: 2.8183,  X1 loss: 2.7978   X2 loss: 2.8388\n","====== Epoch: 236\n","====> Validation loss: 2.8565,  X1 loss: 2.8310   X2 loss: 2.8821\n","====== Epoch: 237\n","====> Validation loss: 2.8189,  X1 loss: 2.8171   X2 loss: 2.8208\n","====== Epoch: 238\n","====> Validation loss: 2.8408,  X1 loss: 2.8255   X2 loss: 2.8561\n","====== Epoch: 239\n","====> Validation loss: 2.8253,  X1 loss: 2.8114   X2 loss: 2.8392\n","====== Epoch: 240\n","====> Validation loss: 2.8724,  X1 loss: 2.8508   X2 loss: 2.8939\n","====== Epoch: 241\n","====> Validation loss: 2.8674,  X1 loss: 2.8519   X2 loss: 2.8829\n","====== Epoch: 242\n","====> Validation loss: 2.7958,  X1 loss: 2.7896   X2 loss: 2.8019\n","====== Epoch: 243\n","====> Validation loss: 2.8095,  X1 loss: 2.7998   X2 loss: 2.8192\n","====== Epoch: 244\n","====> Validation loss: 2.7987,  X1 loss: 2.7926   X2 loss: 2.8049\n","====== Epoch: 245\n","====> Validation loss: 2.8798,  X1 loss: 2.8624   X2 loss: 2.8971\n","====== Epoch: 246\n","====> Validation loss: 2.8124,  X1 loss: 2.8070   X2 loss: 2.8179\n","====== Epoch: 247\n","====> Validation loss: 2.8206,  X1 loss: 2.8139   X2 loss: 2.8273\n","====== Epoch: 248\n","====> Validation loss: 2.8735,  X1 loss: 2.8667   X2 loss: 2.8803\n","====== Epoch: 249\n","====> Validation loss: 2.9246,  X1 loss: 2.8962   X2 loss: 2.9530\n","====== Epoch: 250\n","====> Validation loss: 2.8652,  X1 loss: 2.8526   X2 loss: 2.8777\n","====== Epoch: 251\n","====> Validation loss: 2.8881,  X1 loss: 2.8738   X2 loss: 2.9023\n","====== Epoch: 252\n","====> Validation loss: 2.8023,  X1 loss: 2.7899   X2 loss: 2.8148\n","====== Epoch: 253\n","====> Validation loss: 2.9039,  X1 loss: 2.8812   X2 loss: 2.9267\n","====== Epoch: 254\n","====> Validation loss: 2.8079,  X1 loss: 2.7961   X2 loss: 2.8197\n","====== Epoch: 255\n","====> Validation loss: 2.8332,  X1 loss: 2.8054   X2 loss: 2.8609\n","====== Epoch: 256\n","====> Validation loss: 2.8900,  X1 loss: 2.8631   X2 loss: 2.9169\n","====== Epoch: 257\n","====> Validation loss: 2.7881,  X1 loss: 2.7569   X2 loss: 2.8192\n","====== Epoch: 258\n","====> Validation loss: 2.8742,  X1 loss: 2.8525   X2 loss: 2.8958\n","====== Epoch: 259\n","====> Validation loss: 2.8433,  X1 loss: 2.8327   X2 loss: 2.8539\n","====== Epoch: 260\n","====> Validation loss: 2.8489,  X1 loss: 2.8321   X2 loss: 2.8656\n","====== Epoch: 261\n","====> Validation loss: 2.8549,  X1 loss: 2.8476   X2 loss: 2.8622\n","====== Epoch: 262\n","====> Validation loss: 2.8747,  X1 loss: 2.8662   X2 loss: 2.8833\n","====== Epoch: 263\n","====> Validation loss: 2.8684,  X1 loss: 2.8512   X2 loss: 2.8857\n","====== Epoch: 264\n","====> Validation loss: 2.8806,  X1 loss: 2.8557   X2 loss: 2.9055\n","====== Epoch: 265\n","====> Validation loss: 2.8271,  X1 loss: 2.8145   X2 loss: 2.8397\n","====== Epoch: 266\n","====> Validation loss: 2.8750,  X1 loss: 2.8496   X2 loss: 2.9004\n","====== Epoch: 267\n","====> Validation loss: 2.8085,  X1 loss: 2.7874   X2 loss: 2.8295\n","====== Epoch: 268\n","====> Validation loss: 2.8471,  X1 loss: 2.8373   X2 loss: 2.8568\n","====== Epoch: 269\n","====> Validation loss: 2.7889,  X1 loss: 2.7778   X2 loss: 2.8001\n","====== Epoch: 270\n","====> Validation loss: 2.8248,  X1 loss: 2.8230   X2 loss: 2.8267\n","====== Epoch: 271\n","====> Validation loss: 2.8916,  X1 loss: 2.8699   X2 loss: 2.9134\n","====== Epoch: 272\n","====> Validation loss: 2.8638,  X1 loss: 2.8604   X2 loss: 2.8671\n","====== Epoch: 273\n","====> Validation loss: 2.8334,  X1 loss: 2.8094   X2 loss: 2.8574\n","====== Epoch: 274\n","====> Validation loss: 2.8116,  X1 loss: 2.7923   X2 loss: 2.8309\n","====== Epoch: 275\n","====> Validation loss: 2.8730,  X1 loss: 2.8546   X2 loss: 2.8913\n","====== Epoch: 276\n","====> Validation loss: 2.8195,  X1 loss: 2.7972   X2 loss: 2.8418\n","====== Epoch: 277\n","====> Validation loss: 2.8543,  X1 loss: 2.8366   X2 loss: 2.8719\n","====== Epoch: 278\n","====> Validation loss: 2.8312,  X1 loss: 2.8100   X2 loss: 2.8523\n","====== Epoch: 279\n","====> Validation loss: 2.8315,  X1 loss: 2.8230   X2 loss: 2.8400\n","====== Epoch: 280\n","====> Validation loss: 2.8979,  X1 loss: 2.8975   X2 loss: 2.8982\n","====== Epoch: 281\n","====> Validation loss: 2.8961,  X1 loss: 2.8723   X2 loss: 2.9198\n","====== Epoch: 282\n","====> Validation loss: 2.8253,  X1 loss: 2.8196   X2 loss: 2.8311\n","====== Epoch: 283\n","====> Validation loss: 2.8453,  X1 loss: 2.8322   X2 loss: 2.8585\n","====== Epoch: 284\n","====> Validation loss: 2.8174,  X1 loss: 2.8120   X2 loss: 2.8228\n","====== Epoch: 285\n","====> Validation loss: 2.8027,  X1 loss: 2.8010   X2 loss: 2.8045\n","====== Epoch: 286\n","====> Validation loss: 2.9069,  X1 loss: 2.8809   X2 loss: 2.9330\n","====== Epoch: 287\n","====> Validation loss: 2.8351,  X1 loss: 2.8123   X2 loss: 2.8578\n","====== Epoch: 288\n","====> Validation loss: 2.8547,  X1 loss: 2.8414   X2 loss: 2.8681\n","====== Epoch: 289\n","====> Validation loss: 2.8274,  X1 loss: 2.8120   X2 loss: 2.8428\n","====== Epoch: 290\n","====> Validation loss: 2.8974,  X1 loss: 2.8613   X2 loss: 2.9336\n","====== Epoch: 291\n","====> Validation loss: 2.7902,  X1 loss: 2.7649   X2 loss: 2.8154\n","====== Epoch: 292\n","====> Validation loss: 2.8093,  X1 loss: 2.7970   X2 loss: 2.8215\n","====== Epoch: 293\n","====> Validation loss: 2.8478,  X1 loss: 2.8289   X2 loss: 2.8668\n","====== Epoch: 294\n","====> Validation loss: 2.8858,  X1 loss: 2.8724   X2 loss: 2.8992\n","====== Epoch: 295\n","====> Validation loss: 2.8346,  X1 loss: 2.8204   X2 loss: 2.8487\n","====== Epoch: 296\n","====> Validation loss: 2.7866,  X1 loss: 2.7663   X2 loss: 2.8069\n","====== Epoch: 297\n","====> Validation loss: 2.8353,  X1 loss: 2.8388   X2 loss: 2.8318\n","====== Epoch: 298\n","====> Validation loss: 2.8643,  X1 loss: 2.8471   X2 loss: 2.8815\n","====== Epoch: 299\n","====> Validation loss: 2.8865,  X1 loss: 2.8824   X2 loss: 2.8905\n","====== Epoch: 300\n","====> Validation loss: 2.8551,  X1 loss: 2.8409   X2 loss: 2.8692\n","====== Epoch: 301\n","====> Validation loss: 2.8178,  X1 loss: 2.8014   X2 loss: 2.8343\n","====== Epoch: 302\n","====> Validation loss: 2.8689,  X1 loss: 2.8488   X2 loss: 2.8891\n","====== Epoch: 303\n","====> Validation loss: 2.8611,  X1 loss: 2.8478   X2 loss: 2.8743\n","====== Epoch: 304\n","====> Validation loss: 2.8403,  X1 loss: 2.8209   X2 loss: 2.8597\n","====== Epoch: 305\n","====> Validation loss: 2.8664,  X1 loss: 2.8553   X2 loss: 2.8776\n","====== Epoch: 306\n","====> Validation loss: 2.8645,  X1 loss: 2.8450   X2 loss: 2.8841\n","====== Epoch: 307\n","====> Validation loss: 2.8619,  X1 loss: 2.8512   X2 loss: 2.8725\n","====== Epoch: 308\n","====> Validation loss: 2.8159,  X1 loss: 2.7947   X2 loss: 2.8372\n","====== Epoch: 309\n","====> Validation loss: 2.8935,  X1 loss: 2.8804   X2 loss: 2.9067\n","====== Epoch: 310\n","====> Validation loss: 2.8743,  X1 loss: 2.8609   X2 loss: 2.8877\n","====== Epoch: 311\n","====> Validation loss: 2.8532,  X1 loss: 2.8390   X2 loss: 2.8674\n","====== Epoch: 312\n","====> Validation loss: 2.8572,  X1 loss: 2.8159   X2 loss: 2.8986\n","====== Epoch: 313\n","====> Validation loss: 2.8222,  X1 loss: 2.8042   X2 loss: 2.8403\n","====== Epoch: 314\n","====> Validation loss: 2.8544,  X1 loss: 2.8314   X2 loss: 2.8774\n","====== Epoch: 315\n","====> Validation loss: 2.8708,  X1 loss: 2.8511   X2 loss: 2.8905\n","====== Epoch: 316\n","====> Validation loss: 2.8949,  X1 loss: 2.8812   X2 loss: 2.9085\n","====== Epoch: 317\n","====> Validation loss: 2.8492,  X1 loss: 2.8339   X2 loss: 2.8645\n","====== Epoch: 318\n","====> Validation loss: 2.9023,  X1 loss: 2.8807   X2 loss: 2.9238\n","====== Epoch: 319\n","====> Validation loss: 2.8375,  X1 loss: 2.8215   X2 loss: 2.8535\n","====== Epoch: 320\n","====> Validation loss: 2.8428,  X1 loss: 2.8271   X2 loss: 2.8585\n","====== Epoch: 321\n","====> Validation loss: 2.8601,  X1 loss: 2.8456   X2 loss: 2.8747\n","====== Epoch: 322\n","====> Validation loss: 2.8411,  X1 loss: 2.8355   X2 loss: 2.8467\n","====== Epoch: 323\n","====> Validation loss: 2.8293,  X1 loss: 2.8119   X2 loss: 2.8468\n","====== Epoch: 324\n","====> Validation loss: 2.8908,  X1 loss: 2.8729   X2 loss: 2.9088\n","====== Epoch: 325\n","====> Validation loss: 2.7993,  X1 loss: 2.7946   X2 loss: 2.8040\n","====== Epoch: 326\n","====> Validation loss: 2.9179,  X1 loss: 2.8904   X2 loss: 2.9454\n","====== Epoch: 327\n","====> Validation loss: 2.8422,  X1 loss: 2.8302   X2 loss: 2.8543\n","====== Epoch: 328\n","====> Validation loss: 2.8700,  X1 loss: 2.8483   X2 loss: 2.8916\n","====== Epoch: 329\n","====> Validation loss: 2.7825,  X1 loss: 2.7844   X2 loss: 2.7805\n","====== Epoch: 330\n","====> Validation loss: 2.8935,  X1 loss: 2.8891   X2 loss: 2.8978\n","====== Epoch: 331\n","====> Validation loss: 2.8229,  X1 loss: 2.8209   X2 loss: 2.8250\n","====== Epoch: 332\n","====> Validation loss: 2.8693,  X1 loss: 2.8605   X2 loss: 2.8780\n","====== Epoch: 333\n","====> Validation loss: 2.8629,  X1 loss: 2.8581   X2 loss: 2.8676\n","====== Epoch: 334\n","====> Validation loss: 2.8147,  X1 loss: 2.7907   X2 loss: 2.8387\n","====== Epoch: 335\n","====> Validation loss: 2.8765,  X1 loss: 2.8757   X2 loss: 2.8774\n","====== Epoch: 336\n","====> Validation loss: 2.7878,  X1 loss: 2.7776   X2 loss: 2.7980\n","====== Epoch: 337\n","====> Validation loss: 2.8422,  X1 loss: 2.8299   X2 loss: 2.8545\n","====== Epoch: 338\n","====> Validation loss: 2.8288,  X1 loss: 2.8291   X2 loss: 2.8285\n","====== Epoch: 339\n","====> Validation loss: 2.8804,  X1 loss: 2.8722   X2 loss: 2.8885\n","====== Epoch: 340\n","====> Validation loss: 2.8483,  X1 loss: 2.8456   X2 loss: 2.8511\n","====== Epoch: 341\n","====> Validation loss: 2.8663,  X1 loss: 2.8554   X2 loss: 2.8772\n","====== Epoch: 342\n","====> Validation loss: 2.8126,  X1 loss: 2.7793   X2 loss: 2.8458\n","====== Epoch: 343\n","====> Validation loss: 2.8151,  X1 loss: 2.7943   X2 loss: 2.8360\n","====== Epoch: 344\n","====> Validation loss: 2.8464,  X1 loss: 2.8355   X2 loss: 2.8573\n","====== Epoch: 345\n","====> Validation loss: 2.8630,  X1 loss: 2.8350   X2 loss: 2.8910\n","====== Epoch: 346\n","====> Validation loss: 2.8059,  X1 loss: 2.7988   X2 loss: 2.8131\n","====== Epoch: 347\n","====> Validation loss: 2.9229,  X1 loss: 2.9181   X2 loss: 2.9277\n","====== Epoch: 348\n","====> Validation loss: 2.8546,  X1 loss: 2.8387   X2 loss: 2.8706\n","====== Epoch: 349\n","====> Validation loss: 2.8166,  X1 loss: 2.8171   X2 loss: 2.8162\n","====== Epoch: 350\n","====> Validation loss: 2.9043,  X1 loss: 2.8675   X2 loss: 2.9411\n","====== Epoch: 351\n","====> Validation loss: 2.8072,  X1 loss: 2.7787   X2 loss: 2.8356\n","====== Epoch: 352\n","====> Validation loss: 2.8263,  X1 loss: 2.8163   X2 loss: 2.8363\n","====== Epoch: 353\n","====> Validation loss: 2.8342,  X1 loss: 2.8262   X2 loss: 2.8421\n","====== Epoch: 354\n","====> Validation loss: 2.8932,  X1 loss: 2.8731   X2 loss: 2.9132\n","====== Epoch: 355\n","====> Validation loss: 2.8527,  X1 loss: 2.8264   X2 loss: 2.8791\n","====== Epoch: 356\n","====> Validation loss: 2.8184,  X1 loss: 2.8149   X2 loss: 2.8220\n","====== Epoch: 357\n","====> Validation loss: 2.8191,  X1 loss: 2.8140   X2 loss: 2.8242\n","====== Epoch: 358\n","====> Validation loss: 2.7820,  X1 loss: 2.7590   X2 loss: 2.8050\n","====== Epoch: 359\n","====> Validation loss: 2.8617,  X1 loss: 2.8683   X2 loss: 2.8550\n","====== Epoch: 360\n","====> Validation loss: 2.8021,  X1 loss: 2.7758   X2 loss: 2.8283\n","====== Epoch: 361\n","====> Validation loss: 2.7963,  X1 loss: 2.7876   X2 loss: 2.8051\n","====== Epoch: 362\n","====> Validation loss: 2.8655,  X1 loss: 2.8480   X2 loss: 2.8829\n","====== Epoch: 363\n","====> Validation loss: 2.8223,  X1 loss: 2.7957   X2 loss: 2.8489\n","====== Epoch: 364\n","====> Validation loss: 2.7896,  X1 loss: 2.7650   X2 loss: 2.8143\n","====== Epoch: 365\n","====> Validation loss: 2.8742,  X1 loss: 2.8644   X2 loss: 2.8840\n","====== Epoch: 366\n","====> Validation loss: 2.8267,  X1 loss: 2.8152   X2 loss: 2.8382\n","====== Epoch: 367\n","====> Validation loss: 2.8059,  X1 loss: 2.7939   X2 loss: 2.8179\n","====== Epoch: 368\n","====> Validation loss: 2.8199,  X1 loss: 2.7953   X2 loss: 2.8446\n","====== Epoch: 369\n","====> Validation loss: 2.8725,  X1 loss: 2.8543   X2 loss: 2.8907\n","====== Epoch: 370\n","====> Validation loss: 2.8497,  X1 loss: 2.8236   X2 loss: 2.8759\n","====== Epoch: 371\n","====> Validation loss: 2.7367,  X1 loss: 2.7320   X2 loss: 2.7414\n","====== Epoch: 372\n","====> Validation loss: 2.8233,  X1 loss: 2.8212   X2 loss: 2.8254\n","====== Epoch: 373\n","====> Validation loss: 2.7995,  X1 loss: 2.7899   X2 loss: 2.8091\n","====== Epoch: 374\n","====> Validation loss: 2.8082,  X1 loss: 2.7844   X2 loss: 2.8320\n","====== Epoch: 375\n","====> Validation loss: 2.9253,  X1 loss: 2.9057   X2 loss: 2.9450\n","====== Epoch: 376\n","====> Validation loss: 2.8068,  X1 loss: 2.7914   X2 loss: 2.8222\n","====== Epoch: 377\n","====> Validation loss: 2.7781,  X1 loss: 2.7700   X2 loss: 2.7862\n","====== Epoch: 378\n","====> Validation loss: 2.8396,  X1 loss: 2.8262   X2 loss: 2.8530\n","====== Epoch: 379\n","====> Validation loss: 2.8040,  X1 loss: 2.7934   X2 loss: 2.8146\n","====== Epoch: 380\n","====> Validation loss: 2.7996,  X1 loss: 2.7949   X2 loss: 2.8043\n","====== Epoch: 381\n","====> Validation loss: 2.8785,  X1 loss: 2.8595   X2 loss: 2.8975\n","====== Epoch: 382\n","====> Validation loss: 2.8288,  X1 loss: 2.8123   X2 loss: 2.8454\n","====== Epoch: 383\n","====> Validation loss: 2.8490,  X1 loss: 2.8281   X2 loss: 2.8699\n","====== Epoch: 384\n","====> Validation loss: 2.8729,  X1 loss: 2.8627   X2 loss: 2.8831\n","====== Epoch: 385\n","====> Validation loss: 2.8392,  X1 loss: 2.8103   X2 loss: 2.8680\n","====== Epoch: 386\n","====> Validation loss: 2.8531,  X1 loss: 2.8309   X2 loss: 2.8754\n","====== Epoch: 387\n","====> Validation loss: 2.8177,  X1 loss: 2.8148   X2 loss: 2.8205\n","====== Epoch: 388\n","====> Validation loss: 2.8964,  X1 loss: 2.8779   X2 loss: 2.9150\n","====== Epoch: 389\n","====> Validation loss: 2.8250,  X1 loss: 2.8066   X2 loss: 2.8434\n","====== Epoch: 390\n","====> Validation loss: 2.9219,  X1 loss: 2.9060   X2 loss: 2.9379\n","====== Epoch: 391\n","====> Validation loss: 2.8036,  X1 loss: 2.7974   X2 loss: 2.8098\n","====== Epoch: 392\n","====> Validation loss: 2.8093,  X1 loss: 2.8053   X2 loss: 2.8133\n","====== Epoch: 393\n","====> Validation loss: 2.8212,  X1 loss: 2.8136   X2 loss: 2.8288\n","====== Epoch: 394\n","====> Validation loss: 2.8665,  X1 loss: 2.8494   X2 loss: 2.8837\n","====== Epoch: 395\n","====> Validation loss: 2.8780,  X1 loss: 2.8547   X2 loss: 2.9013\n","====== Epoch: 396\n","====> Validation loss: 2.8176,  X1 loss: 2.7849   X2 loss: 2.8503\n","====== Epoch: 397\n","====> Validation loss: 2.8193,  X1 loss: 2.8040   X2 loss: 2.8346\n","====== Epoch: 398\n","====> Validation loss: 2.8583,  X1 loss: 2.8428   X2 loss: 2.8738\n","====== Epoch: 399\n","====> Validation loss: 2.8270,  X1 loss: 2.8196   X2 loss: 2.8344\n","+--------------New model: eeg0lin_env2conv0lin----------------------+\n","====== Epoch: 1\n","====> Validation loss: 3.3328,  X1 loss: 3.3351   X2 loss: 3.3305\n","====== Epoch: 2\n","====> Validation loss: 3.2389,  X1 loss: 3.2401   X2 loss: 3.2377\n","====== Epoch: 3\n","====> Validation loss: 3.1139,  X1 loss: 3.1161   X2 loss: 3.1117\n","====== Epoch: 4\n","====> Validation loss: 2.9867,  X1 loss: 2.9827   X2 loss: 2.9907\n","====== Epoch: 5\n","====> Validation loss: 3.0193,  X1 loss: 3.0215   X2 loss: 3.0171\n","====== Epoch: 6\n","====> Validation loss: 2.9790,  X1 loss: 2.9717   X2 loss: 2.9863\n","====== Epoch: 7\n","====> Validation loss: 2.9770,  X1 loss: 2.9754   X2 loss: 2.9787\n","====== Epoch: 8\n","====> Validation loss: 2.9789,  X1 loss: 2.9680   X2 loss: 2.9899\n","====== Epoch: 9\n","====> Validation loss: 2.8981,  X1 loss: 2.8849   X2 loss: 2.9114\n","====== Epoch: 10\n","====> Validation loss: 2.9108,  X1 loss: 2.8968   X2 loss: 2.9249\n","====== Epoch: 11\n","====> Validation loss: 2.9729,  X1 loss: 2.9637   X2 loss: 2.9820\n","====== Epoch: 12\n","====> Validation loss: 2.9064,  X1 loss: 2.8873   X2 loss: 2.9255\n","====== Epoch: 13\n","====> Validation loss: 2.8753,  X1 loss: 2.8671   X2 loss: 2.8834\n","====== Epoch: 14\n","====> Validation loss: 2.8796,  X1 loss: 2.8666   X2 loss: 2.8926\n","====== Epoch: 15\n","====> Validation loss: 2.8703,  X1 loss: 2.8513   X2 loss: 2.8893\n","====== Epoch: 16\n","====> Validation loss: 2.8777,  X1 loss: 2.8543   X2 loss: 2.9011\n","====== Epoch: 17\n","====> Validation loss: 2.8437,  X1 loss: 2.8288   X2 loss: 2.8585\n","====== Epoch: 18\n","====> Validation loss: 2.8511,  X1 loss: 2.8364   X2 loss: 2.8659\n","====== Epoch: 19\n","====> Validation loss: 2.8578,  X1 loss: 2.8520   X2 loss: 2.8636\n","====== Epoch: 20\n","====> Validation loss: 2.7576,  X1 loss: 2.7535   X2 loss: 2.7618\n","====== Epoch: 21\n","====> Validation loss: 2.8282,  X1 loss: 2.8103   X2 loss: 2.8461\n","====== Epoch: 22\n","====> Validation loss: 2.8297,  X1 loss: 2.8167   X2 loss: 2.8427\n","====== Epoch: 23\n","====> Validation loss: 2.7801,  X1 loss: 2.7753   X2 loss: 2.7849\n","====== Epoch: 24\n","====> Validation loss: 2.7735,  X1 loss: 2.7644   X2 loss: 2.7826\n","====== Epoch: 25\n","====> Validation loss: 2.8296,  X1 loss: 2.8092   X2 loss: 2.8501\n","====== Epoch: 26\n","====> Validation loss: 2.7980,  X1 loss: 2.7802   X2 loss: 2.8159\n","====== Epoch: 27\n","====> Validation loss: 2.7741,  X1 loss: 2.7612   X2 loss: 2.7869\n","====== Epoch: 28\n","====> Validation loss: 2.7937,  X1 loss: 2.7837   X2 loss: 2.8037\n","====== Epoch: 29\n","====> Validation loss: 2.7887,  X1 loss: 2.7672   X2 loss: 2.8103\n","====== Epoch: 30\n","====> Validation loss: 2.7672,  X1 loss: 2.7665   X2 loss: 2.7680\n","====== Epoch: 31\n","====> Validation loss: 2.7314,  X1 loss: 2.7192   X2 loss: 2.7437\n","====== Epoch: 32\n","====> Validation loss: 2.8184,  X1 loss: 2.8136   X2 loss: 2.8231\n","====== Epoch: 33\n","====> Validation loss: 2.7591,  X1 loss: 2.7438   X2 loss: 2.7744\n","====== Epoch: 34\n","====> Validation loss: 2.7230,  X1 loss: 2.7165   X2 loss: 2.7296\n","====== Epoch: 35\n","====> Validation loss: 2.8188,  X1 loss: 2.8007   X2 loss: 2.8369\n","====== Epoch: 36\n","====> Validation loss: 2.8344,  X1 loss: 2.8166   X2 loss: 2.8522\n","====== Epoch: 37\n","====> Validation loss: 2.8515,  X1 loss: 2.8384   X2 loss: 2.8646\n","====== Epoch: 38\n","====> Validation loss: 2.7695,  X1 loss: 2.7526   X2 loss: 2.7864\n","====== Epoch: 39\n","====> Validation loss: 2.7672,  X1 loss: 2.7561   X2 loss: 2.7783\n","====== Epoch: 40\n","====> Validation loss: 2.7750,  X1 loss: 2.7650   X2 loss: 2.7850\n","====== Epoch: 41\n","====> Validation loss: 2.7775,  X1 loss: 2.7536   X2 loss: 2.8013\n","====== Epoch: 42\n","====> Validation loss: 2.7854,  X1 loss: 2.7716   X2 loss: 2.7991\n","====== Epoch: 43\n","====> Validation loss: 2.7479,  X1 loss: 2.7253   X2 loss: 2.7704\n","====== Epoch: 44\n","====> Validation loss: 2.7281,  X1 loss: 2.7264   X2 loss: 2.7297\n","====== Epoch: 45\n","====> Validation loss: 2.7420,  X1 loss: 2.7272   X2 loss: 2.7567\n","====== Epoch: 46\n","====> Validation loss: 2.7360,  X1 loss: 2.7270   X2 loss: 2.7451\n","====== Epoch: 47\n","====> Validation loss: 2.7355,  X1 loss: 2.7159   X2 loss: 2.7550\n","====== Epoch: 48\n","====> Validation loss: 2.7812,  X1 loss: 2.7657   X2 loss: 2.7966\n","====== Epoch: 49\n","====> Validation loss: 2.7725,  X1 loss: 2.7542   X2 loss: 2.7907\n","====== Epoch: 50\n","====> Validation loss: 2.7582,  X1 loss: 2.7399   X2 loss: 2.7765\n","====== Epoch: 51\n","====> Validation loss: 2.7321,  X1 loss: 2.7134   X2 loss: 2.7508\n","====== Epoch: 52\n","====> Validation loss: 2.7814,  X1 loss: 2.7712   X2 loss: 2.7915\n","====== Epoch: 53\n","====> Validation loss: 2.8041,  X1 loss: 2.7817   X2 loss: 2.8266\n","====== Epoch: 54\n","====> Validation loss: 2.6738,  X1 loss: 2.6644   X2 loss: 2.6833\n","====== Epoch: 55\n","====> Validation loss: 2.7709,  X1 loss: 2.7488   X2 loss: 2.7929\n","====== Epoch: 56\n","====> Validation loss: 2.7744,  X1 loss: 2.7633   X2 loss: 2.7855\n","====== Epoch: 57\n","====> Validation loss: 2.7577,  X1 loss: 2.7515   X2 loss: 2.7639\n","====== Epoch: 58\n","====> Validation loss: 2.7390,  X1 loss: 2.7204   X2 loss: 2.7576\n","====== Epoch: 59\n","====> Validation loss: 2.7408,  X1 loss: 2.7346   X2 loss: 2.7469\n","====== Epoch: 60\n","====> Validation loss: 2.6736,  X1 loss: 2.6646   X2 loss: 2.6827\n","====== Epoch: 61\n","====> Validation loss: 2.6781,  X1 loss: 2.6673   X2 loss: 2.6889\n","====== Epoch: 62\n","====> Validation loss: 2.7426,  X1 loss: 2.7349   X2 loss: 2.7504\n","====== Epoch: 63\n","====> Validation loss: 2.7110,  X1 loss: 2.6938   X2 loss: 2.7282\n","====== Epoch: 64\n","====> Validation loss: 2.7366,  X1 loss: 2.7293   X2 loss: 2.7439\n","====== Epoch: 65\n","====> Validation loss: 2.7384,  X1 loss: 2.7374   X2 loss: 2.7394\n","====== Epoch: 66\n","====> Validation loss: 2.8131,  X1 loss: 2.7973   X2 loss: 2.8289\n","====== Epoch: 67\n","====> Validation loss: 2.7769,  X1 loss: 2.7536   X2 loss: 2.8002\n","====== Epoch: 68\n","====> Validation loss: 2.7832,  X1 loss: 2.7714   X2 loss: 2.7949\n","====== Epoch: 69\n","====> Validation loss: 2.7331,  X1 loss: 2.7206   X2 loss: 2.7456\n","====== Epoch: 70\n","====> Validation loss: 2.7646,  X1 loss: 2.7498   X2 loss: 2.7793\n","====== Epoch: 71\n","====> Validation loss: 2.7614,  X1 loss: 2.7553   X2 loss: 2.7676\n","====== Epoch: 72\n","====> Validation loss: 2.7418,  X1 loss: 2.7309   X2 loss: 2.7528\n","====== Epoch: 73\n","====> Validation loss: 2.7717,  X1 loss: 2.7578   X2 loss: 2.7856\n","====== Epoch: 74\n","====> Validation loss: 2.7560,  X1 loss: 2.7415   X2 loss: 2.7704\n","====== Epoch: 75\n","====> Validation loss: 2.7206,  X1 loss: 2.6944   X2 loss: 2.7467\n","====== Epoch: 76\n","====> Validation loss: 2.8055,  X1 loss: 2.7915   X2 loss: 2.8194\n","====== Epoch: 77\n","====> Validation loss: 2.7207,  X1 loss: 2.7071   X2 loss: 2.7343\n","====== Epoch: 78\n","====> Validation loss: 2.7506,  X1 loss: 2.7498   X2 loss: 2.7514\n","====== Epoch: 79\n","====> Validation loss: 2.6484,  X1 loss: 2.6352   X2 loss: 2.6617\n","====== Epoch: 80\n","====> Validation loss: 2.7023,  X1 loss: 2.6916   X2 loss: 2.7130\n","====== Epoch: 81\n","====> Validation loss: 2.6821,  X1 loss: 2.6780   X2 loss: 2.6861\n","====== Epoch: 82\n","====> Validation loss: 2.7046,  X1 loss: 2.7013   X2 loss: 2.7080\n","====== Epoch: 83\n","====> Validation loss: 2.7356,  X1 loss: 2.7212   X2 loss: 2.7500\n","====== Epoch: 84\n","====> Validation loss: 2.7078,  X1 loss: 2.6930   X2 loss: 2.7226\n","====== Epoch: 85\n","====> Validation loss: 2.7034,  X1 loss: 2.6836   X2 loss: 2.7231\n","====== Epoch: 86\n","====> Validation loss: 2.8137,  X1 loss: 2.8110   X2 loss: 2.8165\n","====== Epoch: 87\n","====> Validation loss: 2.7667,  X1 loss: 2.7514   X2 loss: 2.7819\n","====== Epoch: 88\n","====> Validation loss: 2.7098,  X1 loss: 2.6829   X2 loss: 2.7367\n","====== Epoch: 89\n","====> Validation loss: 2.7431,  X1 loss: 2.7317   X2 loss: 2.7545\n","====== Epoch: 90\n","====> Validation loss: 2.7540,  X1 loss: 2.7362   X2 loss: 2.7717\n","====== Epoch: 91\n","====> Validation loss: 2.7567,  X1 loss: 2.7370   X2 loss: 2.7765\n","====== Epoch: 92\n","====> Validation loss: 2.7101,  X1 loss: 2.6936   X2 loss: 2.7265\n","====== Epoch: 93\n","====> Validation loss: 2.7539,  X1 loss: 2.7421   X2 loss: 2.7657\n","====== Epoch: 94\n","====> Validation loss: 2.7135,  X1 loss: 2.6979   X2 loss: 2.7291\n","====== Epoch: 95\n","====> Validation loss: 2.7120,  X1 loss: 2.7057   X2 loss: 2.7182\n","====== Epoch: 96\n","====> Validation loss: 2.7264,  X1 loss: 2.6970   X2 loss: 2.7559\n","====== Epoch: 97\n","====> Validation loss: 2.7417,  X1 loss: 2.7257   X2 loss: 2.7577\n","====== Epoch: 98\n","====> Validation loss: 2.7884,  X1 loss: 2.7601   X2 loss: 2.8167\n","====== Epoch: 99\n","====> Validation loss: 2.6679,  X1 loss: 2.6649   X2 loss: 2.6709\n","====== Epoch: 100\n","====> Validation loss: 2.6845,  X1 loss: 2.6643   X2 loss: 2.7047\n","====== Epoch: 101\n","====> Validation loss: 2.7139,  X1 loss: 2.6973   X2 loss: 2.7306\n","====== Epoch: 102\n","====> Validation loss: 2.6963,  X1 loss: 2.6913   X2 loss: 2.7013\n","====== Epoch: 103\n","====> Validation loss: 2.7186,  X1 loss: 2.7103   X2 loss: 2.7270\n","====== Epoch: 104\n","====> Validation loss: 2.7538,  X1 loss: 2.7334   X2 loss: 2.7742\n","====== Epoch: 105\n","====> Validation loss: 2.7310,  X1 loss: 2.7128   X2 loss: 2.7493\n","====== Epoch: 106\n","====> Validation loss: 2.7174,  X1 loss: 2.7036   X2 loss: 2.7312\n","====== Epoch: 107\n","====> Validation loss: 2.7300,  X1 loss: 2.7277   X2 loss: 2.7322\n","====== Epoch: 108\n","====> Validation loss: 2.7319,  X1 loss: 2.7258   X2 loss: 2.7380\n","====== Epoch: 109\n","====> Validation loss: 2.6619,  X1 loss: 2.6526   X2 loss: 2.6712\n","====== Epoch: 110\n","====> Validation loss: 2.7146,  X1 loss: 2.7057   X2 loss: 2.7236\n","====== Epoch: 111\n","====> Validation loss: 2.7115,  X1 loss: 2.7057   X2 loss: 2.7174\n","====== Epoch: 112\n","====> Validation loss: 2.7000,  X1 loss: 2.6801   X2 loss: 2.7198\n","====== Epoch: 113\n","====> Validation loss: 2.6784,  X1 loss: 2.6618   X2 loss: 2.6950\n","====== Epoch: 114\n","====> Validation loss: 2.6552,  X1 loss: 2.6453   X2 loss: 2.6651\n","====== Epoch: 115\n","====> Validation loss: 2.7173,  X1 loss: 2.7077   X2 loss: 2.7269\n","====== Epoch: 116\n","====> Validation loss: 2.7103,  X1 loss: 2.6933   X2 loss: 2.7273\n","====== Epoch: 117\n","====> Validation loss: 2.6388,  X1 loss: 2.6232   X2 loss: 2.6545\n","====== Epoch: 118\n","====> Validation loss: 2.6812,  X1 loss: 2.6681   X2 loss: 2.6944\n","====== Epoch: 119\n","====> Validation loss: 2.6791,  X1 loss: 2.6655   X2 loss: 2.6926\n","====== Epoch: 120\n","====> Validation loss: 2.7734,  X1 loss: 2.7598   X2 loss: 2.7870\n","====== Epoch: 121\n","====> Validation loss: 2.7337,  X1 loss: 2.7192   X2 loss: 2.7482\n","====== Epoch: 122\n","====> Validation loss: 2.6440,  X1 loss: 2.6420   X2 loss: 2.6459\n","====== Epoch: 123\n","====> Validation loss: 2.7127,  X1 loss: 2.7043   X2 loss: 2.7210\n","====== Epoch: 124\n","====> Validation loss: 2.6831,  X1 loss: 2.6655   X2 loss: 2.7007\n","====== Epoch: 125\n","====> Validation loss: 2.7217,  X1 loss: 2.7079   X2 loss: 2.7356\n","====== Epoch: 126\n","====> Validation loss: 2.6572,  X1 loss: 2.6438   X2 loss: 2.6705\n","====== Epoch: 127\n","====> Validation loss: 2.7079,  X1 loss: 2.6994   X2 loss: 2.7164\n","====== Epoch: 128\n","====> Validation loss: 2.7218,  X1 loss: 2.7125   X2 loss: 2.7311\n","====== Epoch: 129\n","====> Validation loss: 2.7346,  X1 loss: 2.7241   X2 loss: 2.7450\n","====== Epoch: 130\n","====> Validation loss: 2.6943,  X1 loss: 2.6693   X2 loss: 2.7194\n","====== Epoch: 131\n","====> Validation loss: 2.6670,  X1 loss: 2.6602   X2 loss: 2.6738\n","====== Epoch: 132\n","====> Validation loss: 2.6797,  X1 loss: 2.6519   X2 loss: 2.7075\n","====== Epoch: 133\n","====> Validation loss: 2.6736,  X1 loss: 2.6691   X2 loss: 2.6782\n","====== Epoch: 134\n","====> Validation loss: 2.6596,  X1 loss: 2.6387   X2 loss: 2.6805\n","====== Epoch: 135\n","====> Validation loss: 2.6954,  X1 loss: 2.6987   X2 loss: 2.6922\n","====== Epoch: 136\n","====> Validation loss: 2.7080,  X1 loss: 2.7037   X2 loss: 2.7123\n","====== Epoch: 137\n","====> Validation loss: 2.7673,  X1 loss: 2.7502   X2 loss: 2.7845\n","====== Epoch: 138\n","====> Validation loss: 2.6920,  X1 loss: 2.6770   X2 loss: 2.7071\n","====== Epoch: 139\n","====> Validation loss: 2.7528,  X1 loss: 2.7480   X2 loss: 2.7576\n","====== Epoch: 140\n","====> Validation loss: 2.7055,  X1 loss: 2.6943   X2 loss: 2.7166\n","====== Epoch: 141\n","====> Validation loss: 2.6597,  X1 loss: 2.6497   X2 loss: 2.6696\n","====== Epoch: 142\n","====> Validation loss: 2.7217,  X1 loss: 2.7120   X2 loss: 2.7315\n","====== Epoch: 143\n","====> Validation loss: 2.7154,  X1 loss: 2.7053   X2 loss: 2.7255\n","====== Epoch: 144\n","====> Validation loss: 2.6940,  X1 loss: 2.6971   X2 loss: 2.6909\n","====== Epoch: 145\n","====> Validation loss: 2.7153,  X1 loss: 2.7237   X2 loss: 2.7069\n","====== Epoch: 146\n","====> Validation loss: 2.7023,  X1 loss: 2.6935   X2 loss: 2.7112\n","====== Epoch: 147\n","====> Validation loss: 2.7402,  X1 loss: 2.7482   X2 loss: 2.7323\n","====== Epoch: 148\n","====> Validation loss: 2.7567,  X1 loss: 2.7532   X2 loss: 2.7602\n","====== Epoch: 149\n","====> Validation loss: 2.7088,  X1 loss: 2.7078   X2 loss: 2.7097\n","====== Epoch: 150\n","====> Validation loss: 2.6703,  X1 loss: 2.6455   X2 loss: 2.6951\n","====== Epoch: 151\n","====> Validation loss: 2.6949,  X1 loss: 2.6781   X2 loss: 2.7118\n","====== Epoch: 152\n","====> Validation loss: 2.6519,  X1 loss: 2.6468   X2 loss: 2.6569\n","====== Epoch: 153\n","====> Validation loss: 2.7560,  X1 loss: 2.7442   X2 loss: 2.7678\n","====== Epoch: 154\n","====> Validation loss: 2.7287,  X1 loss: 2.7201   X2 loss: 2.7373\n","====== Epoch: 155\n","====> Validation loss: 2.7226,  X1 loss: 2.7100   X2 loss: 2.7353\n","====== Epoch: 156\n","====> Validation loss: 2.7444,  X1 loss: 2.7505   X2 loss: 2.7384\n","====== Epoch: 157\n","====> Validation loss: 2.7851,  X1 loss: 2.7700   X2 loss: 2.8002\n","====== Epoch: 158\n","====> Validation loss: 2.7460,  X1 loss: 2.7334   X2 loss: 2.7587\n","====== Epoch: 159\n","====> Validation loss: 2.7637,  X1 loss: 2.7415   X2 loss: 2.7858\n","====== Epoch: 160\n","====> Validation loss: 2.7538,  X1 loss: 2.7388   X2 loss: 2.7687\n","====== Epoch: 161\n","====> Validation loss: 2.7140,  X1 loss: 2.7008   X2 loss: 2.7271\n","====== Epoch: 162\n","====> Validation loss: 2.7020,  X1 loss: 2.6964   X2 loss: 2.7076\n","====== Epoch: 163\n","====> Validation loss: 2.7346,  X1 loss: 2.7314   X2 loss: 2.7378\n","====== Epoch: 164\n","====> Validation loss: 2.7497,  X1 loss: 2.7285   X2 loss: 2.7710\n","====== Epoch: 165\n","====> Validation loss: 2.6585,  X1 loss: 2.6424   X2 loss: 2.6746\n","====== Epoch: 166\n","====> Validation loss: 2.7263,  X1 loss: 2.7044   X2 loss: 2.7481\n","====== Epoch: 167\n","====> Validation loss: 2.7022,  X1 loss: 2.6983   X2 loss: 2.7062\n","====== Epoch: 168\n","====> Validation loss: 2.6652,  X1 loss: 2.6590   X2 loss: 2.6714\n","====== Epoch: 169\n","====> Validation loss: 2.6951,  X1 loss: 2.6854   X2 loss: 2.7048\n","====== Epoch: 170\n","====> Validation loss: 2.7397,  X1 loss: 2.7320   X2 loss: 2.7473\n","====== Epoch: 171\n","====> Validation loss: 2.7080,  X1 loss: 2.7115   X2 loss: 2.7045\n","====== Epoch: 172\n","====> Validation loss: 2.7092,  X1 loss: 2.7132   X2 loss: 2.7053\n","====== Epoch: 173\n","====> Validation loss: 2.7788,  X1 loss: 2.7698   X2 loss: 2.7878\n","====== Epoch: 174\n","====> Validation loss: 2.7169,  X1 loss: 2.7008   X2 loss: 2.7330\n","====== Epoch: 175\n","====> Validation loss: 2.6844,  X1 loss: 2.6709   X2 loss: 2.6979\n","====== Epoch: 176\n","====> Validation loss: 2.7114,  X1 loss: 2.7039   X2 loss: 2.7189\n","====== Epoch: 177\n","====> Validation loss: 2.7844,  X1 loss: 2.7647   X2 loss: 2.8041\n","====== Epoch: 178\n","====> Validation loss: 2.7112,  X1 loss: 2.7118   X2 loss: 2.7106\n","====== Epoch: 179\n","====> Validation loss: 2.7403,  X1 loss: 2.7234   X2 loss: 2.7571\n","====== Epoch: 180\n","====> Validation loss: 2.6491,  X1 loss: 2.6431   X2 loss: 2.6550\n","====== Epoch: 181\n","====> Validation loss: 2.7276,  X1 loss: 2.7140   X2 loss: 2.7411\n","====== Epoch: 182\n","====> Validation loss: 2.7186,  X1 loss: 2.7267   X2 loss: 2.7105\n","====== Epoch: 183\n","====> Validation loss: 2.7386,  X1 loss: 2.7391   X2 loss: 2.7381\n","====== Epoch: 184\n","====> Validation loss: 2.6812,  X1 loss: 2.6597   X2 loss: 2.7028\n","====== Epoch: 185\n","====> Validation loss: 2.7674,  X1 loss: 2.7377   X2 loss: 2.7970\n","====== Epoch: 186\n","====> Validation loss: 2.6817,  X1 loss: 2.6668   X2 loss: 2.6966\n","====== Epoch: 187\n","====> Validation loss: 2.6864,  X1 loss: 2.6833   X2 loss: 2.6895\n","====== Epoch: 188\n","====> Validation loss: 2.7543,  X1 loss: 2.7427   X2 loss: 2.7659\n","====== Epoch: 189\n","====> Validation loss: 2.7091,  X1 loss: 2.6932   X2 loss: 2.7251\n","====== Epoch: 190\n","====> Validation loss: 2.7243,  X1 loss: 2.7196   X2 loss: 2.7290\n","====== Epoch: 191\n","====> Validation loss: 2.7736,  X1 loss: 2.7697   X2 loss: 2.7776\n","====== Epoch: 192\n","====> Validation loss: 2.6887,  X1 loss: 2.6858   X2 loss: 2.6917\n","====== Epoch: 193\n","====> Validation loss: 2.6993,  X1 loss: 2.7012   X2 loss: 2.6975\n","====== Epoch: 194\n","====> Validation loss: 2.7130,  X1 loss: 2.7060   X2 loss: 2.7200\n","====== Epoch: 195\n","====> Validation loss: 2.8056,  X1 loss: 2.7752   X2 loss: 2.8361\n","====== Epoch: 196\n","====> Validation loss: 2.7255,  X1 loss: 2.7254   X2 loss: 2.7257\n","====== Epoch: 197\n","====> Validation loss: 2.6873,  X1 loss: 2.6677   X2 loss: 2.7068\n","====== Epoch: 198\n","====> Validation loss: 2.6937,  X1 loss: 2.7027   X2 loss: 2.6847\n","====== Epoch: 199\n","====> Validation loss: 2.6965,  X1 loss: 2.6856   X2 loss: 2.7074\n","====== Epoch: 200\n","====> Validation loss: 2.7682,  X1 loss: 2.7550   X2 loss: 2.7814\n","====== Epoch: 201\n","====> Validation loss: 2.7309,  X1 loss: 2.7129   X2 loss: 2.7488\n","====== Epoch: 202\n","====> Validation loss: 2.6790,  X1 loss: 2.6745   X2 loss: 2.6836\n","====== Epoch: 203\n","====> Validation loss: 2.7104,  X1 loss: 2.7020   X2 loss: 2.7188\n","====== Epoch: 204\n","====> Validation loss: 2.6652,  X1 loss: 2.6559   X2 loss: 2.6744\n","====== Epoch: 205\n","====> Validation loss: 2.7180,  X1 loss: 2.6984   X2 loss: 2.7376\n","====== Epoch: 206\n","====> Validation loss: 2.7505,  X1 loss: 2.7487   X2 loss: 2.7522\n","====== Epoch: 207\n","====> Validation loss: 2.7022,  X1 loss: 2.6863   X2 loss: 2.7181\n","====== Epoch: 208\n","====> Validation loss: 2.7362,  X1 loss: 2.7234   X2 loss: 2.7489\n","====== Epoch: 209\n","====> Validation loss: 2.7406,  X1 loss: 2.7181   X2 loss: 2.7630\n","====== Epoch: 210\n","====> Validation loss: 2.7334,  X1 loss: 2.7275   X2 loss: 2.7393\n","====== Epoch: 211\n","====> Validation loss: 2.7102,  X1 loss: 2.7025   X2 loss: 2.7179\n","====== Epoch: 212\n","====> Validation loss: 2.7225,  X1 loss: 2.7020   X2 loss: 2.7430\n","====== Epoch: 213\n","====> Validation loss: 2.7545,  X1 loss: 2.7371   X2 loss: 2.7719\n","====== Epoch: 214\n","====> Validation loss: 2.7363,  X1 loss: 2.7207   X2 loss: 2.7519\n","====== Epoch: 215\n","====> Validation loss: 2.6816,  X1 loss: 2.6633   X2 loss: 2.7000\n","====== Epoch: 216\n","====> Validation loss: 2.7252,  X1 loss: 2.7138   X2 loss: 2.7365\n","====== Epoch: 217\n","====> Validation loss: 2.7029,  X1 loss: 2.7052   X2 loss: 2.7007\n","====== Epoch: 218\n","====> Validation loss: 2.6825,  X1 loss: 2.6761   X2 loss: 2.6890\n","====== Epoch: 219\n","====> Validation loss: 2.7203,  X1 loss: 2.7164   X2 loss: 2.7242\n","====== Epoch: 220\n","====> Validation loss: 2.7356,  X1 loss: 2.7371   X2 loss: 2.7341\n","====== Epoch: 221\n","====> Validation loss: 2.7071,  X1 loss: 2.6856   X2 loss: 2.7286\n","====== Epoch: 222\n","====> Validation loss: 2.6598,  X1 loss: 2.6599   X2 loss: 2.6598\n","====== Epoch: 223\n","====> Validation loss: 2.7444,  X1 loss: 2.7410   X2 loss: 2.7478\n","====== Epoch: 224\n","====> Validation loss: 2.6916,  X1 loss: 2.6789   X2 loss: 2.7043\n","====== Epoch: 225\n","====> Validation loss: 2.7461,  X1 loss: 2.7020   X2 loss: 2.7901\n","====== Epoch: 226\n","====> Validation loss: 2.6938,  X1 loss: 2.6904   X2 loss: 2.6972\n","====== Epoch: 227\n","====> Validation loss: 2.6768,  X1 loss: 2.6658   X2 loss: 2.6878\n","====== Epoch: 228\n","====> Validation loss: 2.6886,  X1 loss: 2.6927   X2 loss: 2.6844\n","====== Epoch: 229\n","====> Validation loss: 2.6623,  X1 loss: 2.6564   X2 loss: 2.6682\n","====== Epoch: 230\n","====> Validation loss: 2.7241,  X1 loss: 2.7084   X2 loss: 2.7398\n","====== Epoch: 231\n","====> Validation loss: 2.7255,  X1 loss: 2.7122   X2 loss: 2.7388\n","====== Epoch: 232\n","====> Validation loss: 2.7411,  X1 loss: 2.7336   X2 loss: 2.7487\n","====== Epoch: 233\n","====> Validation loss: 2.7499,  X1 loss: 2.7464   X2 loss: 2.7533\n","====== Epoch: 234\n","====> Validation loss: 2.7543,  X1 loss: 2.7390   X2 loss: 2.7697\n","====== Epoch: 235\n","====> Validation loss: 2.7284,  X1 loss: 2.7115   X2 loss: 2.7452\n","====== Epoch: 236\n","====> Validation loss: 2.7762,  X1 loss: 2.7726   X2 loss: 2.7797\n","====== Epoch: 237\n","====> Validation loss: 2.7155,  X1 loss: 2.7129   X2 loss: 2.7180\n","====== Epoch: 238\n","====> Validation loss: 2.6101,  X1 loss: 2.5910   X2 loss: 2.6291\n","====== Epoch: 239\n","====> Validation loss: 2.7602,  X1 loss: 2.7503   X2 loss: 2.7701\n","====== Epoch: 240\n","====> Validation loss: 2.7468,  X1 loss: 2.7397   X2 loss: 2.7538\n","====== Epoch: 241\n","====> Validation loss: 2.6637,  X1 loss: 2.6573   X2 loss: 2.6701\n","====== Epoch: 242\n","====> Validation loss: 2.6665,  X1 loss: 2.6538   X2 loss: 2.6792\n","====== Epoch: 243\n","====> Validation loss: 2.7019,  X1 loss: 2.6825   X2 loss: 2.7212\n","====== Epoch: 244\n","====> Validation loss: 2.7587,  X1 loss: 2.7428   X2 loss: 2.7747\n","====== Epoch: 245\n","====> Validation loss: 2.6909,  X1 loss: 2.6746   X2 loss: 2.7073\n","====== Epoch: 246\n","====> Validation loss: 2.6775,  X1 loss: 2.6618   X2 loss: 2.6932\n","====== Epoch: 247\n","====> Validation loss: 2.6993,  X1 loss: 2.6894   X2 loss: 2.7093\n","====== Epoch: 248\n","====> Validation loss: 2.7190,  X1 loss: 2.6936   X2 loss: 2.7443\n","====== Epoch: 249\n","====> Validation loss: 2.7338,  X1 loss: 2.7220   X2 loss: 2.7456\n","====== Epoch: 250\n","====> Validation loss: 2.7343,  X1 loss: 2.7271   X2 loss: 2.7415\n","====== Epoch: 251\n","====> Validation loss: 2.6986,  X1 loss: 2.6772   X2 loss: 2.7200\n","====== Epoch: 252\n","====> Validation loss: 2.7331,  X1 loss: 2.7243   X2 loss: 2.7420\n","====== Epoch: 253\n","====> Validation loss: 2.7514,  X1 loss: 2.7354   X2 loss: 2.7675\n","====== Epoch: 254\n","====> Validation loss: 2.7033,  X1 loss: 2.6923   X2 loss: 2.7143\n","====== Epoch: 255\n","====> Validation loss: 2.7767,  X1 loss: 2.7641   X2 loss: 2.7893\n","====== Epoch: 256\n","====> Validation loss: 2.7496,  X1 loss: 2.7430   X2 loss: 2.7562\n","====== Epoch: 257\n","====> Validation loss: 2.7595,  X1 loss: 2.7495   X2 loss: 2.7694\n","====== Epoch: 258\n","====> Validation loss: 2.6525,  X1 loss: 2.6394   X2 loss: 2.6655\n","====== Epoch: 259\n","====> Validation loss: 2.6678,  X1 loss: 2.6676   X2 loss: 2.6680\n","====== Epoch: 260\n","====> Validation loss: 2.6762,  X1 loss: 2.6632   X2 loss: 2.6893\n","====== Epoch: 261\n","====> Validation loss: 2.6844,  X1 loss: 2.6864   X2 loss: 2.6824\n","====== Epoch: 262\n","====> Validation loss: 2.7188,  X1 loss: 2.7087   X2 loss: 2.7289\n","====== Epoch: 263\n","====> Validation loss: 2.7273,  X1 loss: 2.7080   X2 loss: 2.7466\n","====== Epoch: 264\n","====> Validation loss: 2.7621,  X1 loss: 2.7474   X2 loss: 2.7768\n","====== Epoch: 265\n","====> Validation loss: 2.7535,  X1 loss: 2.7313   X2 loss: 2.7758\n","====== Epoch: 266\n","====> Validation loss: 2.6953,  X1 loss: 2.6925   X2 loss: 2.6980\n","====== Epoch: 267\n","====> Validation loss: 2.7460,  X1 loss: 2.7470   X2 loss: 2.7449\n","====== Epoch: 268\n","====> Validation loss: 2.7233,  X1 loss: 2.7142   X2 loss: 2.7324\n","====== Epoch: 269\n","====> Validation loss: 2.6937,  X1 loss: 2.6776   X2 loss: 2.7098\n","====== Epoch: 270\n","====> Validation loss: 2.7661,  X1 loss: 2.7593   X2 loss: 2.7730\n","====== Epoch: 271\n","====> Validation loss: 2.7117,  X1 loss: 2.7015   X2 loss: 2.7218\n","====== Epoch: 272\n","====> Validation loss: 2.6988,  X1 loss: 2.6827   X2 loss: 2.7149\n","====== Epoch: 273\n","====> Validation loss: 2.7646,  X1 loss: 2.7606   X2 loss: 2.7687\n","====== Epoch: 274\n","====> Validation loss: 2.7375,  X1 loss: 2.7288   X2 loss: 2.7463\n","====== Epoch: 275\n","====> Validation loss: 2.7149,  X1 loss: 2.6984   X2 loss: 2.7314\n","====== Epoch: 276\n","====> Validation loss: 2.7088,  X1 loss: 2.6885   X2 loss: 2.7292\n","====== Epoch: 277\n","====> Validation loss: 2.7380,  X1 loss: 2.7263   X2 loss: 2.7498\n","====== Epoch: 278\n","====> Validation loss: 2.7587,  X1 loss: 2.7227   X2 loss: 2.7946\n","====== Epoch: 279\n","====> Validation loss: 2.7157,  X1 loss: 2.7003   X2 loss: 2.7312\n","====== Epoch: 280\n","====> Validation loss: 2.7319,  X1 loss: 2.7115   X2 loss: 2.7523\n","====== Epoch: 281\n","====> Validation loss: 2.6869,  X1 loss: 2.6660   X2 loss: 2.7078\n","====== Epoch: 282\n","====> Validation loss: 2.6788,  X1 loss: 2.6570   X2 loss: 2.7005\n","====== Epoch: 283\n","====> Validation loss: 2.7016,  X1 loss: 2.6980   X2 loss: 2.7052\n","====== Epoch: 284\n","====> Validation loss: 2.7129,  X1 loss: 2.7018   X2 loss: 2.7240\n","====== Epoch: 285\n","====> Validation loss: 2.7787,  X1 loss: 2.7647   X2 loss: 2.7928\n","====== Epoch: 286\n","====> Validation loss: 2.6862,  X1 loss: 2.6838   X2 loss: 2.6885\n","====== Epoch: 287\n","====> Validation loss: 2.7037,  X1 loss: 2.6878   X2 loss: 2.7196\n","====== Epoch: 288\n","====> Validation loss: 2.7190,  X1 loss: 2.7067   X2 loss: 2.7313\n","====== Epoch: 289\n","====> Validation loss: 2.7718,  X1 loss: 2.7539   X2 loss: 2.7898\n","====== Epoch: 290\n","====> Validation loss: 2.6932,  X1 loss: 2.6753   X2 loss: 2.7111\n","====== Epoch: 291\n","====> Validation loss: 2.6466,  X1 loss: 2.6451   X2 loss: 2.6481\n","====== Epoch: 292\n","====> Validation loss: 2.6378,  X1 loss: 2.6403   X2 loss: 2.6353\n","====== Epoch: 293\n","====> Validation loss: 2.7320,  X1 loss: 2.7431   X2 loss: 2.7210\n","====== Epoch: 294\n","====> Validation loss: 2.6879,  X1 loss: 2.6987   X2 loss: 2.6771\n","====== Epoch: 295\n","====> Validation loss: 2.6780,  X1 loss: 2.6777   X2 loss: 2.6783\n","====== Epoch: 296\n","====> Validation loss: 2.7075,  X1 loss: 2.6981   X2 loss: 2.7169\n","====== Epoch: 297\n","====> Validation loss: 2.6974,  X1 loss: 2.6916   X2 loss: 2.7031\n","====== Epoch: 298\n","====> Validation loss: 2.6727,  X1 loss: 2.6745   X2 loss: 2.6710\n","====== Epoch: 299\n","====> Validation loss: 2.7121,  X1 loss: 2.6925   X2 loss: 2.7316\n","====== Epoch: 300\n","====> Validation loss: 2.6801,  X1 loss: 2.6728   X2 loss: 2.6875\n","====== Epoch: 301\n","====> Validation loss: 2.6928,  X1 loss: 2.6766   X2 loss: 2.7090\n","====== Epoch: 302\n","====> Validation loss: 2.6293,  X1 loss: 2.6125   X2 loss: 2.6461\n","====== Epoch: 303\n","====> Validation loss: 2.7778,  X1 loss: 2.7692   X2 loss: 2.7865\n","====== Epoch: 304\n","====> Validation loss: 2.7573,  X1 loss: 2.7263   X2 loss: 2.7883\n","====== Epoch: 305\n","====> Validation loss: 2.6922,  X1 loss: 2.6869   X2 loss: 2.6975\n","====== Epoch: 306\n","====> Validation loss: 2.6713,  X1 loss: 2.6569   X2 loss: 2.6857\n","====== Epoch: 307\n","====> Validation loss: 2.7292,  X1 loss: 2.7066   X2 loss: 2.7518\n","====== Epoch: 308\n","====> Validation loss: 2.7170,  X1 loss: 2.7109   X2 loss: 2.7232\n","====== Epoch: 309\n","====> Validation loss: 2.6906,  X1 loss: 2.6777   X2 loss: 2.7036\n","====== Epoch: 310\n","====> Validation loss: 2.7431,  X1 loss: 2.7427   X2 loss: 2.7436\n","====== Epoch: 311\n","====> Validation loss: 2.6981,  X1 loss: 2.7007   X2 loss: 2.6954\n","====== Epoch: 312\n","====> Validation loss: 2.7146,  X1 loss: 2.6953   X2 loss: 2.7340\n","====== Epoch: 313\n","====> Validation loss: 2.7162,  X1 loss: 2.7221   X2 loss: 2.7104\n","====== Epoch: 314\n","====> Validation loss: 2.7604,  X1 loss: 2.7524   X2 loss: 2.7684\n","====== Epoch: 315\n","====> Validation loss: 2.6892,  X1 loss: 2.6913   X2 loss: 2.6872\n","====== Epoch: 316\n","====> Validation loss: 2.7821,  X1 loss: 2.7725   X2 loss: 2.7917\n","====== Epoch: 317\n","====> Validation loss: 2.7071,  X1 loss: 2.6945   X2 loss: 2.7196\n","====== Epoch: 318\n","====> Validation loss: 2.7146,  X1 loss: 2.7014   X2 loss: 2.7277\n","====== Epoch: 319\n","====> Validation loss: 2.7373,  X1 loss: 2.7289   X2 loss: 2.7458\n","====== Epoch: 320\n","====> Validation loss: 2.7068,  X1 loss: 2.6899   X2 loss: 2.7237\n","====== Epoch: 321\n","====> Validation loss: 2.7149,  X1 loss: 2.7059   X2 loss: 2.7239\n","====== Epoch: 322\n","====> Validation loss: 2.7291,  X1 loss: 2.7035   X2 loss: 2.7548\n","====== Epoch: 323\n","====> Validation loss: 2.7232,  X1 loss: 2.7083   X2 loss: 2.7381\n","====== Epoch: 324\n","====> Validation loss: 2.7163,  X1 loss: 2.7117   X2 loss: 2.7210\n","====== Epoch: 325\n","====> Validation loss: 2.7353,  X1 loss: 2.7216   X2 loss: 2.7491\n","====== Epoch: 326\n","====> Validation loss: 2.6735,  X1 loss: 2.6679   X2 loss: 2.6792\n","====== Epoch: 327\n","====> Validation loss: 2.7277,  X1 loss: 2.7123   X2 loss: 2.7430\n","====== Epoch: 328\n","====> Validation loss: 2.6735,  X1 loss: 2.6638   X2 loss: 2.6831\n","====== Epoch: 329\n","====> Validation loss: 2.7242,  X1 loss: 2.7110   X2 loss: 2.7375\n","====== Epoch: 330\n","====> Validation loss: 2.7550,  X1 loss: 2.7400   X2 loss: 2.7699\n","====== Epoch: 331\n","====> Validation loss: 2.7652,  X1 loss: 2.7561   X2 loss: 2.7743\n","====== Epoch: 332\n","====> Validation loss: 2.6635,  X1 loss: 2.6628   X2 loss: 2.6642\n","====== Epoch: 333\n","====> Validation loss: 2.7514,  X1 loss: 2.7339   X2 loss: 2.7688\n","====== Epoch: 334\n","====> Validation loss: 2.6542,  X1 loss: 2.6373   X2 loss: 2.6710\n","====== Epoch: 335\n","====> Validation loss: 2.6969,  X1 loss: 2.6780   X2 loss: 2.7158\n","====== Epoch: 336\n","====> Validation loss: 2.7207,  X1 loss: 2.7146   X2 loss: 2.7267\n","====== Epoch: 337\n","====> Validation loss: 2.7298,  X1 loss: 2.7269   X2 loss: 2.7327\n","====== Epoch: 338\n","====> Validation loss: 2.7234,  X1 loss: 2.7023   X2 loss: 2.7445\n","====== Epoch: 339\n","====> Validation loss: 2.7414,  X1 loss: 2.7362   X2 loss: 2.7465\n","====== Epoch: 340\n","====> Validation loss: 2.7053,  X1 loss: 2.6928   X2 loss: 2.7178\n","====== Epoch: 341\n","====> Validation loss: 2.6508,  X1 loss: 2.6513   X2 loss: 2.6503\n","====== Epoch: 342\n","====> Validation loss: 2.6897,  X1 loss: 2.6823   X2 loss: 2.6972\n","====== Epoch: 343\n","====> Validation loss: 2.6951,  X1 loss: 2.6856   X2 loss: 2.7046\n","====== Epoch: 344\n","====> Validation loss: 2.7192,  X1 loss: 2.7099   X2 loss: 2.7286\n","====== Epoch: 345\n","====> Validation loss: 2.7030,  X1 loss: 2.6869   X2 loss: 2.7191\n","====== Epoch: 346\n","====> Validation loss: 2.6728,  X1 loss: 2.6692   X2 loss: 2.6764\n","====== Epoch: 347\n","====> Validation loss: 2.7365,  X1 loss: 2.7121   X2 loss: 2.7609\n","====== Epoch: 348\n","====> Validation loss: 2.7097,  X1 loss: 2.6946   X2 loss: 2.7248\n","====== Epoch: 349\n","====> Validation loss: 2.6444,  X1 loss: 2.6433   X2 loss: 2.6454\n","====== Epoch: 350\n","====> Validation loss: 2.7038,  X1 loss: 2.6896   X2 loss: 2.7179\n","====== Epoch: 351\n","====> Validation loss: 2.7098,  X1 loss: 2.7039   X2 loss: 2.7157\n","====== Epoch: 352\n","====> Validation loss: 2.7415,  X1 loss: 2.7216   X2 loss: 2.7614\n","====== Epoch: 353\n","====> Validation loss: 2.7392,  X1 loss: 2.7240   X2 loss: 2.7544\n","====== Epoch: 354\n","====> Validation loss: 2.7532,  X1 loss: 2.7518   X2 loss: 2.7546\n","====== Epoch: 355\n","====> Validation loss: 2.7251,  X1 loss: 2.7180   X2 loss: 2.7322\n","====== Epoch: 356\n","====> Validation loss: 2.7984,  X1 loss: 2.7902   X2 loss: 2.8066\n","====== Epoch: 357\n","====> Validation loss: 2.6607,  X1 loss: 2.6437   X2 loss: 2.6777\n","====== Epoch: 358\n","====> Validation loss: 2.7766,  X1 loss: 2.7616   X2 loss: 2.7917\n","====== Epoch: 359\n","====> Validation loss: 2.7053,  X1 loss: 2.6993   X2 loss: 2.7113\n","====== Epoch: 360\n","====> Validation loss: 2.6931,  X1 loss: 2.6666   X2 loss: 2.7196\n","====== Epoch: 361\n","====> Validation loss: 2.6028,  X1 loss: 2.5879   X2 loss: 2.6176\n","====== Epoch: 362\n","====> Validation loss: 2.7290,  X1 loss: 2.7175   X2 loss: 2.7404\n","====== Epoch: 363\n","====> Validation loss: 2.6791,  X1 loss: 2.6663   X2 loss: 2.6920\n","====== Epoch: 364\n","====> Validation loss: 2.7039,  X1 loss: 2.6916   X2 loss: 2.7162\n","====== Epoch: 365\n","====> Validation loss: 2.6480,  X1 loss: 2.6356   X2 loss: 2.6604\n","====== Epoch: 366\n","====> Validation loss: 2.6629,  X1 loss: 2.6440   X2 loss: 2.6818\n","====== Epoch: 367\n","====> Validation loss: 2.7141,  X1 loss: 2.7017   X2 loss: 2.7264\n","====== Epoch: 368\n","====> Validation loss: 2.6755,  X1 loss: 2.6600   X2 loss: 2.6910\n","====== Epoch: 369\n","====> Validation loss: 2.6925,  X1 loss: 2.6797   X2 loss: 2.7053\n","====== Epoch: 370\n","====> Validation loss: 2.6856,  X1 loss: 2.6763   X2 loss: 2.6949\n","====== Epoch: 371\n","====> Validation loss: 2.7292,  X1 loss: 2.7184   X2 loss: 2.7399\n","====== Epoch: 372\n","====> Validation loss: 2.7229,  X1 loss: 2.7160   X2 loss: 2.7298\n","====== Epoch: 373\n","====> Validation loss: 2.7265,  X1 loss: 2.7118   X2 loss: 2.7412\n","====== Epoch: 374\n","====> Validation loss: 2.6608,  X1 loss: 2.6565   X2 loss: 2.6651\n","====== Epoch: 375\n","====> Validation loss: 2.7583,  X1 loss: 2.7496   X2 loss: 2.7670\n","====== Epoch: 376\n","====> Validation loss: 2.6787,  X1 loss: 2.6623   X2 loss: 2.6951\n","====== Epoch: 377\n","====> Validation loss: 2.6570,  X1 loss: 2.6380   X2 loss: 2.6760\n","====== Epoch: 378\n","====> Validation loss: 2.6973,  X1 loss: 2.6949   X2 loss: 2.6996\n","====== Epoch: 379\n","====> Validation loss: 2.6237,  X1 loss: 2.6045   X2 loss: 2.6430\n","====== Epoch: 380\n","====> Validation loss: 2.7014,  X1 loss: 2.6881   X2 loss: 2.7148\n","====== Epoch: 381\n","====> Validation loss: 2.7276,  X1 loss: 2.7119   X2 loss: 2.7432\n","====== Epoch: 382\n","====> Validation loss: 2.7059,  X1 loss: 2.7014   X2 loss: 2.7104\n","====== Epoch: 383\n","====> Validation loss: 2.6771,  X1 loss: 2.6774   X2 loss: 2.6768\n","====== Epoch: 384\n","====> Validation loss: 2.6380,  X1 loss: 2.6262   X2 loss: 2.6498\n","====== Epoch: 385\n","====> Validation loss: 2.7167,  X1 loss: 2.7040   X2 loss: 2.7294\n","====== Epoch: 386\n","====> Validation loss: 2.7443,  X1 loss: 2.7316   X2 loss: 2.7570\n","====== Epoch: 387\n","====> Validation loss: 2.6942,  X1 loss: 2.6810   X2 loss: 2.7073\n","====== Epoch: 388\n","====> Validation loss: 2.7309,  X1 loss: 2.7306   X2 loss: 2.7313\n","====== Epoch: 389\n","====> Validation loss: 2.6525,  X1 loss: 2.6357   X2 loss: 2.6692\n","====== Epoch: 390\n","====> Validation loss: 2.7321,  X1 loss: 2.7118   X2 loss: 2.7525\n","====== Epoch: 391\n","====> Validation loss: 2.7349,  X1 loss: 2.7447   X2 loss: 2.7251\n","====== Epoch: 392\n","====> Validation loss: 2.6793,  X1 loss: 2.6750   X2 loss: 2.6835\n","====== Epoch: 393\n","====> Validation loss: 2.7724,  X1 loss: 2.7534   X2 loss: 2.7915\n","====== Epoch: 394\n","====> Validation loss: 2.7220,  X1 loss: 2.7053   X2 loss: 2.7386\n","====== Epoch: 395\n","====> Validation loss: 2.6514,  X1 loss: 2.6422   X2 loss: 2.6607\n","====== Epoch: 396\n","====> Validation loss: 2.7183,  X1 loss: 2.7082   X2 loss: 2.7284\n","====== Epoch: 397\n","====> Validation loss: 2.6977,  X1 loss: 2.6865   X2 loss: 2.7089\n","====== Epoch: 398\n","====> Validation loss: 2.7052,  X1 loss: 2.6936   X2 loss: 2.7168\n","====== Epoch: 399\n"]}],"source":["\n","lossi = []\n","udri = [] # update / data ratio \n","ud = []\n","\n","lr = 0.001\n","\n","for name, model in models_dict.items():\n","\n","    # Reset for the new model in the loop\n","    print(f\"+--------------New model: {name}----------------------+\")\n","    writer = SummaryWriter(log_dir=f\"runs/{name}_{time.strftime('%Y%m%d_%H%M%S')}\")\n","    model.to(device)\n","    optimizer = optim.NAdam(model.parameters(), lr=lr)\n","    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=200, gamma=0.1)\n","    cnt = 0\n","    loss_batches = []\n","\n","\n","    for epoch in range(1, 400):\n","\n","        print(f\"====== Epoch: {epoch}\")\n","\n","        model.train()\n","        for ix_batch, (Xb_eeg, Xb_env) in enumerate(dataloader):\n","\n","            # send to device\n","            Xb_eeg = Xb_eeg.to(device)\n","            Xb_env = Xb_env.to(device)\n","\n","            # Zero out gradients\n","            optimizer.zero_grad()\n","\n","            # forward pass\n","            eeg_features, env_features, logit_scale = model(Xb_eeg, Xb_env) \n","\n","\n","            # normalize features\n","            eeg_features_n = eeg_features / eeg_features.norm(dim=1, keepdim=True)\n","            env_features_n = env_features / env_features.norm(dim=1, keepdim=True)\n","\n","            # logits\n","            logits_per_eeg = logit_scale * eeg_features_n @ env_features_n.t()\n","            logits_per_env = logits_per_eeg.t()\n","\n","            #loss function\n","            labels = torch.arange(batch_size).to(device)\n","            loss_eeg = F.cross_entropy(logits_per_eeg, labels)\n","            loss_env = F.cross_entropy(logits_per_env, labels)\n","            loss   = (loss_eeg + loss_env)/2\n","\n","            # backward pass\n","            loss.backward()\n","            optimizer.step()\n","\n","            loss_batches.append(loss.item())\n","            cnt += 1\n","\n","            with torch.no_grad():\n","                #ud = {f\"p{ix}\":(lr*p.grad.std() / p.data.std()).log10().item() for ix, p in enumerate(model.parameters()) if p.ndim==4 }\n","                #writer.add_scalars('UpdateOData/ud', ud, cnt)\n","                writer.add_scalar('Loss/train_batch', loss.item(), cnt)\n","\n","            # normalize weights\n","            with torch.no_grad():\n","                normalize_weights_eegnet(model.eeg_encoder)\n","            \n","            #break   \n","\n","        loss_epoch = loss_batches[-(ix_batch + 1):]  # mean loss across batches\n","        loss_epoch = sum(loss_epoch) / len(loss_epoch)\n","        writer.add_scalar('Loss/train_epoch', loss_epoch, epoch)\n","        #for pname, p in model.named_parameters():\n","        #writer.add_histogram(f'Params/{pname}', p, epoch)\n","        #writer.add_histogram(f'Grads/{pname}', p.grad, epoch)\n","\n","        loss_val, *_ = eval_model_cl(dl_val, model, device=device)\n","        writer.add_scalar('Loss/val_epoch', loss_val, epoch)\n","\n","        \n","\n","        model.train()\n","\n","        # Update learning rate based on epoch\n","        scheduler.step()\n","            \n","    #break   \n"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[{"file_id":"13Poz5tFnEFXpegMbhqAinRdIfSvPnPge","timestamp":1677524195292}]},"gpuClass":"premium","kernelspec":{"display_name":"mne","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"8e19e54895c02f0e9343d0fbd6cee45458aaf6f05de9ab3004d10bba5525a5d0"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"18b03f99d4eb4147a85fb45f03670b42":{"model_module":"@jupyter-widgets/output","model_name":"OutputModel","model_module_version":"1.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/output","_model_module_version":"1.0.0","_model_name":"OutputModel","_view_count":null,"_view_module":"@jupyter-widgets/output","_view_module_version":"1.0.0","_view_name":"OutputView","layout":"IPY_MODEL_80bf39e46c8846a191e3250accc813ef","msg_id":"","outputs":[{"output_type":"display_data","data":{"text/plain":"     \u001b[38;2;249;38;114m\u001b[0m\u001b[38;2;249;38;114m\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">     <span style=\"color: #f92672; text-decoration-color: #f92672\"></span> <span style=\"color: #008000; text-decoration-color: #008000\">7.6/7.6 MB</span> <span style=\"color: #800000; text-decoration-color: #800000\">67.8 MB/s</span> eta <span style=\"color: #008080; text-decoration-color: #008080\">0:00:01</span>\n</pre>\n"},"metadata":{}}]}},"80bf39e46c8846a191e3250accc813ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}